<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dynamicio.mixins.with_kafka API documentation</title>
<meta name="description" content="This module provides mixins that are providing Kafka I/O support." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dynamicio.mixins.with_kafka</code></h1>
</header>
<section id="section-intro">
<p>This module provides mixins that are providing Kafka I/O support.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;This module provides mixins that are providing Kafka I/O support.&#34;&#34;&#34;
# pylint: disable=no-member, protected-access, too-few-public-methods

from typing import Any, Callable, Mapping, MutableMapping, Optional

import pandas as pd  # type: ignore
import simplejson
from confluent_kafka import Producer
from magic_logger import logger

from dynamicio.config.pydantic import DataframeSchema, KafkaDataEnvironment
from dynamicio.mixins import utils


class WithKafka:
    &#34;&#34;&#34;Handles I/O operations for Kafka.

    Args:
        - options:
            - Standard: Keyword-arguments passed to the KafkaProducer constructor (see `KafkaProducer.DEFAULT_CONFIG.keys()`).
             - Additional Options:

                - `key_generator: Callable[[Any, Mapping], T]`: defines the keying policy to be used for sending keyed-messages to Kafka. It is a `Callable` that takes a
                `tuple(idx, row)` and returns a string that will serve as the message&#39;s key, invoked prior to serialising the key. It defaults to the dataframe&#39;s index
                (which may not be composed of unique values or string type keys). It goes hand in hand with the default `key-serialiser`, which assumes that the keys
                are strings and encode&#39;s them as such.

                - `key_serializer: Callable[T, bytes]`: Custom key serialiser; if not provided, a default key-serializer will be used, applied on a string-key (unless key is None).

                N.B. Providing a custom key-generator that generates a non-string key is best provided alongside a custom key-serializer best suited to handle the custom key-type.

                - `document_transformer: Callable[[Mapping[Any, Any]`: Manipulates the messages/rows sent to Kafka as values. It is  a `Callable` taking a `Mapping` as its only
                argument and return a `Mapping`, then this callable will be invoked prior to serializing each document. This can be used, for example, to add metadata to each
                document that will be written to the target  Kafka topic.

                - `value_serializer: Callable[Mapping, bytes]`: Custom value serialiser; if not provided, a default value-serializer will be used applied on a Mapping..

    Example:
        &gt;&gt;&gt; # Given
        &gt;&gt;&gt; keyed_test_df = pd.DataFrame.from_records(
        &gt;&gt;&gt;     [
        &gt;&gt;&gt;         [&#34;key-01&#34;, &#34;cm_1&#34;, &#34;id_1&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;         [&#34;key-02&#34;, &#34;cm_2&#34;, &#34;id_2&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;         [&#34;key-03&#34;, &#34;cm_3&#34;, &#34;id_3&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;     ],
        &gt;&gt;&gt;     columns=[&#34;key&#34;, &#34;id&#34;, &#34;foo&#34;, &#34;bar&#34;, &#34;baz&#34;],
        &gt;&gt;&gt; ).set_index(&#34;key&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; kafka_cloud_config = IOConfig(
        &gt;&gt;&gt;     path_to_source_yaml=(os.path.join(constants.TEST_RESOURCES, &#34;processed.yaml&#34;)),
        &gt;&gt;&gt;     env_identifier=&#34;CLOUD&#34;,
        &gt;&gt;&gt;     dynamic_vars=constants,
        &gt;&gt;&gt; ).get(source_key=&#34;WRITE_TO_KAFKA_JSON&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; write_kafka_io = WriteKafkaIO(kafka_cloud_config, key_generator=lambda key, _: key, document_transformer=lambda doc: doc[&#34;new_field&#34;]=&#34;new_value&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; # When
        &gt;&gt;&gt; with patch.object(mixins, &#34;KafkaProducer&#34;) as mock__kafka_producer:
        &gt;&gt;&gt;     mock__kafka_producer.DEFAULT_CONFIG = KafkaProducer.DEFAULT_CONFIG
        &gt;&gt;&gt;     mock_producer = MockKafkaProducer()
        &gt;&gt;&gt;     mock__kafka_producer.return_value = mock_producer
        &gt;&gt;&gt;     write_kafka_io.write(keyed_test_df)
        &gt;&gt;&gt;
        &gt;&gt;&gt; # Then
        &gt;&gt;&gt; assert mock_producer.my_stream == [
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-01&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_1&#34;, &#34;id&#34;: &#34;cm_1&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-02&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_2&#34;, &#34;id&#34;: &#34;cm_2&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-03&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_3&#34;, &#34;id&#34;: &#34;cm_3&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt; ]
    &#34;&#34;&#34;

    sources_config: KafkaDataEnvironment
    schema: DataframeSchema
    options: MutableMapping[str, Any]
    __kafka_config: Optional[Mapping] = None
    __producer: Optional[Producer] = None
    __key_generator: Optional[Callable[[Any, Mapping[Any, Any]], Optional[str]]] = None
    __document_transformer: Optional[Callable[[Mapping[Any, Any]], Mapping[Any, Any]]] = None
    __key_serializer: Optional[Callable[[Optional[str]], Optional[bytes]]] = None
    __value_serializer: Optional[Callable[[Mapping[Any, Any]], bytes]] = None

    # N.B.: Please refer to https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md and update this config in case of a major release change.
    VALID_CONFIG_KEYS = {
        &#34;acks&#34;,
        &#34;allow.auto.create.topics&#34;,
        &#34;api.version.fallback.ms&#34;,
        &#34;api.version.request.timeout.ms&#34;,
        &#34;api.version.request&#34;,
        &#34;auto.commit.enable&#34;,
        &#34;auto.commit.interval.ms&#34;,
        &#34;auto.offset.reset&#34;,
        &#34;background_event_cb&#34;,
        &#34;batch.size&#34;,
        &#34;batch.num.messages&#34;,
        &#34;bootstrap.servers&#34;,
        &#34;broker.address.family&#34;,
        &#34;broker.address.ttl&#34;,
        &#34;broker.version.fallback&#34;,
        &#34;builtin.features&#34;,
        &#34;check.crcs&#34;,
        &#34;client.dns.lookup&#34;,
        &#34;client.id&#34;,
        &#34;client.rack&#34;,
        &#34;closesocket_cb&#34;,
        &#34;compression.codec&#34;,
        &#34;compression.level&#34;,
        &#34;compression.type&#34;,
        &#34;connect_cb&#34;,
        &#34;connections.max.idle.ms&#34;,
        &#34;consume_cb&#34;,
        &#34;consume.callback.max.messages&#34;,
        &#34;coordinator.query.interval.ms&#34;,
        &#34;debug&#34;,
        &#34;default_topic_conf&#34;,
        &#34;delivery.report.only.error&#34;,
        &#34;dr_cb&#34;,
        &#34;dr_msg_cb&#34;,
        &#34;enable.auto.commit&#34;,
        &#34;enable.auto.offset.store&#34;,
        &#34;enable.gapless.guarantee&#34;,
        &#34;enable.idempotence&#34;,
        &#34;enable.partition.eof&#34;,
        &#34;enable.random.seed&#34;,
        &#34;enable.sasl.oauthbearer.unsecure.jwt&#34;,
        &#34;enable.ssl.certificate.verification&#34;,
        &#34;enabled_events&#34;,
        &#34;error_cb&#34;,
        &#34;fetch.error.backoff.ms&#34;,
        &#34;fetch.max.bytes&#34;,
        &#34;fetch.message.max.bytes&#34;,
        &#34;fetch.min.bytes&#34;,
        &#34;fetch.queue.backoff.ms&#34;,
        &#34;fetch.wait.max.ms&#34;,
        &#34;group.id&#34;,
        &#34;group.instance.id&#34;,
        &#34;group.protocol.type&#34;,
        &#34;group.protocol&#34;,
        &#34;group.remote.assignor&#34;,
        &#34;heartbeat.interval.ms&#34;,
        &#34;interceptors&#34;,
        &#34;internal.termination.signal&#34;,
        &#34;isolation.level&#34;,
        &#34;linger.ms&#34;,
        &#34;log_cb&#34;,
        &#34;log_level&#34;,
        &#34;log.connection.close&#34;,
        &#34;log.queue&#34;,
        &#34;log.thread.name&#34;,
        &#34;max.block.ms&#34;,
        &#34;max.in.flight.requests.per.connection&#34;,
        &#34;max.in.flight&#34;,
        &#34;max.partition.fetch.bytes&#34;,
        &#34;max.poll.interval.ms&#34;,
        &#34;max.request.size&#34;,
        &#34;message.copy.max.bytes&#34;,
        &#34;message.max.bytes&#34;,
        &#34;message.send.max.retries&#34;,
        &#34;metadata.broker.list&#34;,
        &#34;metadata.max.age.ms&#34;,
        &#34;msg_order_cmp&#34;,
        &#34;oauth_cb&#34;,
        &#34;oauthbearer_token_refresh_cb&#34;,
        &#34;offset_commit_cb&#34;,
        &#34;offset.store.method&#34;,
        &#34;offset.store.path&#34;,
        &#34;offset.store.sync.interval.ms&#34;,
        &#34;on_delivery&#34;,
        &#34;opaque&#34;,
        &#34;open_cb&#34;,
        &#34;partition.assignment.strategy&#34;,
        &#34;partitioner_cb&#34;,
        &#34;partitioner&#34;,
        &#34;plugin.library.paths&#34;,
        &#34;produce.offset.report&#34;,
        &#34;queue.buffering.backpressure.threshold&#34;,
        &#34;queue.buffering.max.kbytes&#34;,
        &#34;queue.buffering.max.messages&#34;,
        &#34;queue.buffering.max.ms&#34;,
        &#34;queued.max.messages.kbytes&#34;,
        &#34;queued.min.messages&#34;,
        &#34;rebalance_cb&#34;,
        &#34;receive.buffer.bytes&#34;,
        &#34;receive.message.max.bytes&#34;,
        &#34;reconnect.backoff.jitter.ms&#34;,
        &#34;reconnect.backoff.max.ms&#34;,
        &#34;reconnect.backoff.ms&#34;,
        &#34;request.timeout.ms&#34;,
        &#34;resolve_cb&#34;,
        &#34;retry.backoff.ms&#34;,
        &#34;sasl.kerberos.keytab&#34;,
        &#34;sasl.kerberos.kinit.cmd&#34;,
        &#34;sasl.kerberos.min.time.before.relogin&#34;,
        &#34;sasl.kerberos.principal&#34;,
        &#34;sasl.kerberos.service.name&#34;,
        &#34;sasl.mechanisms&#34;,
        &#34;sasl.oauthbearer.client.id&#34;,
        &#34;sasl.oauthbearer.client.secret&#34;,
        &#34;sasl.oauthbearer.config&#34;,
        &#34;sasl.oauthbearer.extensions&#34;,
        &#34;sasl.oauthbearer.method&#34;,
        &#34;sasl.oauthbearer.scope&#34;,
        &#34;sasl.oauthbearer.token.endpoint.url&#34;,
        &#34;sasl.password&#34;,
        &#34;sasl.username&#34;,
        &#34;security.protocol&#34;,
        &#34;send.buffer.bytes&#34;,
        &#34;session.timeout.ms&#34;,
        &#34;socket_cb&#34;,
        &#34;socket.blocking.max.ms&#34;,
        &#34;socket.connection.setup.timeout.ms&#34;,
        &#34;socket.keepalive.enable&#34;,
        &#34;socket.max.fails&#34;,
        &#34;socket.nagle.disable&#34;,
        &#34;socket.timeout.ms&#34;,
        &#34;ssl_engine_callback_data&#34;,
        &#34;ssl.ca.certificate.stores&#34;,
        &#34;ssl.ca.location&#34;,
        &#34;ssl.certificate.location&#34;,
        &#34;ssl.certificate.verify_cb&#34;,
        &#34;ssl.cipher.suites&#34;,
        &#34;ssl.crl.location&#34;,
        &#34;ssl.curves.list&#34;,
        &#34;ssl.endpoint.identification.algorithm&#34;,
        &#34;ssl.engine.id&#34;,
        &#34;ssl.engine.location&#34;,
        &#34;ssl.key.location&#34;,
        &#34;ssl.key.password&#34;,
        &#34;ssl.keystore.location&#34;,
        &#34;ssl.keystore.password&#34;,
        &#34;ssl.providers&#34;,
        &#34;ssl.sigalgs.list&#34;,
        &#34;statistics.interval.ms&#34;,
        &#34;sticky.partitioning.linger.ms&#34;,
        &#34;throttle_cb&#34;,
        &#34;topic.blacklist&#34;,
        &#34;topic.metadata.propagation.max.ms&#34;,
        &#34;topic.metadata.refresh.fast.cnt&#34;,
        &#34;topic.metadata.refresh.fast.interval.ms&#34;,
        &#34;topic.metadata.refresh.interval.ms&#34;,
        &#34;topic.metadata.refresh.sparse&#34;,
        &#34;transaction.timeout.ms&#34;,
        &#34;transactional.id&#34;,
    }

    def _write_to_kafka(self, df: pd.DataFrame) -&gt; None:
        &#34;&#34;&#34;Given a dataframe where each row is a message to be sent to a Kafka Topic, iterate through all rows and send them to a Kafka topic.

         The topic is defined in `self.sources_config[&#34;kafka&#34;]` and using a kafka producer, which is flushed at the
         end of this process.

        Args:
            df: A dataframe where each row is a message to be sent to a Kafka Topic.
        &#34;&#34;&#34;
        self.populate_cls_attributes()

        if self.__producer is None:
            self.__producer = self._get_producer(self.sources_config.kafka.kafka_server, **self.options)

        self._send_messages(df=df, topic=self.sources_config.kafka.kafka_topic)

    def populate_cls_attributes(self):
        &#34;&#34;&#34;Pop dynamicio options (key_generator, document_transformer, key_serializer, value_serializer) from kafka config options.&#34;&#34;&#34;
        if self.__key_generator is None:
            self.__key_generator = lambda idx, __: idx  # default key generator uses the dataframe&#39;s index
            if self.options.get(&#34;key_generator&#34;) is not None:
                self.__key_generator = self.options.pop(&#34;key_generator&#34;)
        if self.__document_transformer is None:
            self.__document_transformer = lambda value: value
            if self.options.get(&#34;document_transformer&#34;) is not None:
                self.__document_transformer = self.options.pop(&#34;document_transformer&#34;)
        if self.__key_serializer is None:
            self.__key_serializer = self._default_key_serializer
            if self.options.get(&#34;key_serializer&#34;) is not None:
                self.__key_serializer = self.options.pop(&#34;key_serializer&#34;)
        if self.__value_serializer is None:
            self.__value_serializer = self._default_value_serializer
            if self.options.get(&#34;value_serializer&#34;) is not None:
                self.__value_serializer = self.options.pop(&#34;value_serializer&#34;)

    @utils.allow_options(VALID_CONFIG_KEYS)
    def _get_producer(self, server: str, **options: MutableMapping[str, Any]) -&gt; Producer:
        &#34;&#34;&#34;Generate and return a Kafka Producer.

        Default options are used to generate the producer. Specifically:
            - `bootstrap.servers`: Passed on through the source_config
            - `compression.type`: Uses snappy compression

        More options can be added to the producer by passing them as keyword arguments, through valid options.

        These can also override the default options.

        Args:
            server: The host name.
            **options: Keyword arguments to pass to the KafkaProducer.

        Returns:
            A Kafka producer instance.
        &#34;&#34;&#34;
        self.__kafka_config = {
            &#34;bootstrap.servers&#34;: server,
            &#34;compression.type&#34;: &#34;snappy&#34;,
            **options,
        }
        return Producer(**self.__kafka_config)

    def _send_messages(self, df: pd.DataFrame, topic: str) -&gt; None:
        logger.debug(f&#34;Sending {len(df)} messages to Kafka topic: {topic}.&#34;)
        messages = df.reset_index(drop=True).to_dict(&#34;records&#34;)

        for idx, message in zip(df.index.values, messages):
            key = self.__key_generator(idx, message)
            transformed_message = self.__document_transformer(message)
            serialized_key = self.__key_serializer(key)
            serialized_value = self.__value_serializer(transformed_message)

            self.__producer.produce(topic=topic, key=serialized_key, value=serialized_value, on_delivery=self._on_delivery)

        self.__producer.flush()

    @staticmethod
    def _on_delivery(err, msg):
        &#34;&#34;&#34;Callback for message delivery.&#34;&#34;&#34;
        if err is not None:
            raise Exception(f&#34;Message delivery failed: {err}, for message: {msg}&#34;)

    @staticmethod
    def _default_key_serializer(key: Optional[Any]) -&gt; Optional[bytes]:
        if key is not None:
            return str(key).encode(&#34;utf-8&#34;)
        return None

    @staticmethod
    def _default_value_serializer(value: Mapping) -&gt; bytes:
        return simplejson.dumps(value, ignore_nan=True).encode(&#34;utf-8&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dynamicio.mixins.with_kafka.Producer"><code class="flex name class">
<span>class <span class="ident">Producer</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Asynchronous Kafka Producer</p>
<p>.. py:function:: Producer(config)</p>
<p>:param dict config: Configuration properties. At a minimum <code>bootstrap.servers</code> <strong>should</strong> be set</p>
<p>Create a new Producer instance using the provided configuration dict.</p>
<p>.. py:function:: <strong>len</strong>(self)</p>
<p>Producer implements <strong>len</strong> that can be used as len(producer) to obtain number of messages waiting.
:returns: Number of messages and Kafka protocol requests waiting to be delivered to broker.
:rtype: int</p></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li>confluent_kafka.serializing_producer.SerializingProducer</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dynamicio.mixins.with_kafka.Producer.abort_transaction"><code class="name flex">
<span>def <span class="ident">abort_transaction</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: abort_transaction([timeout])</p>
<p>Aborts the current transaction.
This function should also be used to recover from non-fatal
abortable transaction errors when KafkaError.txn_requires_abort()
is True.</p>
<p>Any outstanding messages will be purged and fail with
_PURGE_INFLIGHT or _PURGE_QUEUE.</p>
<p>Note: This function will block until all outstanding messages
are purged and the transaction abort request has been
successfully handled by the transaction coordinator, or until
the timeout expires, which ever comes first. On timeout the
application may call the function again.</p>
<p>Note: Will automatically call purge() and flush()
to ensure
all queued and in-flight messages are purged before attempting
to abort the transaction.</p>
<p>:param float timeout: The maximum amount of time to block
waiting for transaction to abort in seconds.</p>
<p>:raises: KafkaError: Use exc.args[0].retriable() to check if the
operation may be retried.
Treat any other error as a fatal error.</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.begin_transaction"><code class="name flex">
<span>def <span class="ident">begin_transaction</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: begin_transaction()</p>
<p>Begin a new transaction.</p>
<p>init_transactions() must have been called successfully (once)
before this function is called.</p>
<p>Any messages produced or offsets sent to a transaction, after
the successful return of this function will be part of the
transaction and committed or aborted atomically.</p>
<p>Complete the transaction by calling commit_transaction() or
Abort the transaction by calling abort_transaction().</p>
<p>:raises: KafkaError: Use exc.args[0].retriable() to check if the
operation may be retried, else treat the
error as a fatal error.</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.commit_transaction"><code class="name flex">
<span>def <span class="ident">commit_transaction</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: commit_transaction([timeout])</p>
<p>Commmit the current transaction.
Any outstanding messages will be flushed (delivered) before
actually committing the transaction.</p>
<p>If any of the outstanding messages fail permanently the current
transaction will enter the abortable error state and this
function will return an abortable error, in this case the
application must call abort_transaction() before attempting
a new transaction with begin_transaction().</p>
<p>Note: This function will block until all outstanding messages
are delivered and the transaction commit request has been
successfully handled by the transaction coordinator, or until
the timeout expires, which ever comes first. On timeout the
application may call the function again.</p>
<p>Note: Will automatically call flush() to ensure all queued
messages are delivered before attempting to commit the
transaction. Delivery reports and other callbacks may thus be
triggered from this method.</p>
<p>:param float timeout: The amount of time to block in seconds.</p>
<p>:raises: KafkaError: Use exc.args[0].retriable() to check if the
operation may be retried, or
exc.args[0].txn_requires_abort() if the current
transaction has failed and must be aborted by calling
abort_transaction() and then start a new transaction
with begin_transaction().
Treat any other error as a fatal error.</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.flush"><code class="name flex">
<span>def <span class="ident">flush</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: flush([timeout])</p>
<p>Wait for all messages in the Producer queue to be delivered.
This is a convenience method that calls :py:func:<code>poll()</code> until :py:func:<code>len()</code> is zero or the optional timeout elapses.</p>
<p>:param: float timeout: Maximum time to block (requires librdkafka &gt;= v0.9.4). (Seconds)
:returns: Number of messages still in queue.</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;See :py:func:<code>poll()</code> for a description on what callbacks may be triggered.</p>
</div></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.init_transactions"><code class="name flex">
<span>def <span class="ident">init_transactions</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function: init_transactions([timeout])</p>
<p>Initializes transactions for the producer instance.</p>
<p>This function ensures any transactions initiated by previous
instances of the producer with the same <code>transactional.id</code> are
completed.
If the previous instance failed with a transaction in progress
the previous transaction will be aborted.
This function needs to be called before any other transactional
or produce functions are called when the <code>transactional.id</code> is
configured.</p>
<p>If the last transaction had begun completion (following
transaction commit) but not yet finished, this function will
await the previous transaction's completion.</p>
<p>When any previous transactions have been fenced this function
will acquire the internal producer id and epoch, used in all
future transactional messages issued by this producer instance.</p>
<p>Upon successful return from this function the application has to
perform at least one of the following operations within
<code>transaction.timeout.ms</code> to avoid timing out the transaction
on the broker:
* produce() (et.al)
* send_offsets_to_transaction()
* commit_transaction()
* abort_transaction()</p>
<p>:param float timeout: Maximum time to block in seconds.</p>
<p>:raises: KafkaError: Use exc.args[0].retriable() to check if the
operation may be retried, else treat the
error as a fatal error.</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.list_topics"><code class="name flex">
<span>def <span class="ident">list_topics</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: list_topics([topic=None], [timeout=-1])</p>
<p>Request metadata from the cluster.
This method provides the same information as
listTopics(), describeTopics() and describeCluster() in
the Java Admin client.</p>
<p>:param str topic: If specified, only request information about this topic, else return results for all topics in cluster. Warning: If auto.create.topics.enable is set to true on the broker and an unknown topic is specified, it will be created.
:param float timeout: The maximum response time before timing out, or -1 for infinite timeout.
:rtype: ClusterMetadata
:raises: KafkaException</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.poll"><code class="name flex">
<span>def <span class="ident">poll</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: poll([timeout])</p>
<p>Polls the producer for events and calls the corresponding callbacks (if registered).</p>
<p>Callbacks:</p>
<ul>
<li><code>on_delivery</code> callbacks from :py:func:<code>produce()</code></li>
<li>&hellip;</li>
</ul>
<p>:param float timeout: Maximum time to block waiting for events. (Seconds)
:returns: Number of events processed (callbacks served)
:rtype: int</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.produce"><code class="name flex">
<span>def <span class="ident">produce</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: produce(topic, [value], [key], [partition], [on_delivery], [timestamp], [headers])</p>
<p>Produce message to topic.
This is an asynchronous operation, an application may use the <code>callback</code> (alias <code>on_delivery</code>) argument to pass a function (or lambda) that will be called from :py:func:<code>poll()</code> when the message has been successfully delivered or permanently fails delivery.</p>
<p>Currently message headers are not supported on the message returned to the callback. The <code>msg.headers()</code> will return None even if the original message had headers set.</p>
<p>:param str topic: Topic to produce message to
:param str|bytes value: Message payload
:param str|bytes key: Message key
:param int partition: Partition to produce to, else uses the configured built-in partitioner.
:param func on_delivery(err,msg): Delivery report callback to call (from :py:func:<code>poll()</code> or :py:func:<code>flush()</code>) on successful or failed delivery
:param int timestamp: Message timestamp (CreateTime) in milliseconds since epoch UTC (requires librdkafka &gt;= v0.9.4, api.version.request=true, and broker &gt;= 0.10.0.0). Default value is current time.</p>
<p>:param dict|list headers: Message headers to set on the message. The header key must be a string while the value must be binary, unicode or None. Accepts a list of (key,value) or a dict. (Requires librdkafka &gt;= v0.11.4 and broker version &gt;= 0.11.0.0)
:rtype: None
:raises BufferError: if the internal producer message queue is full (<code>queue.buffering.max.messages</code> exceeded)
:raises KafkaException: for other errors, see exception code
:raises NotImplementedError: if timestamp is specified without underlying library support.</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.purge"><code class="name flex">
<span>def <span class="ident">purge</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: purge([in_queue=True], [in_flight=True], [blocking=True])</p>
<p>Purge messages currently handled by the producer instance.
The application will need to call poll() or flush() afterwards to serve the delivery report callbacks of the purged messages.</p>
<p>:param: bool in_queue: Purge messages from internal queues. By default, true.
:param: bool in_flight: Purge messages in flight to or from the broker. By default, true.
:param: bool blocking: If set to False, will not wait on background thread queue purging to finish. By default, true.</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.send_offsets_to_transaction"><code class="name flex">
<span>def <span class="ident">send_offsets_to_transaction</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: send_offsets_to_transaction(positions, group_metadata, [timeout])</p>
<p>Sends a list of topic partition offsets to the consumer group
coordinator for group_metadata and marks the offsets as part
of the current transaction.
These offsets will be considered committed only if the
transaction is committed successfully.</p>
<p>The offsets should be the next message your application will
consume, i.e., the last processed message's offset + 1 for each
partition.
Either track the offsets manually during processing or use
consumer.position() (on the consumer) to get the current offsets
for the partitions assigned to the consumer.</p>
<p>Use this method at the end of a consume-transform-produce loop
prior to committing the transaction with commit_transaction().</p>
<p>Note: The consumer must disable auto commits
(set <code>enable.auto.commit</code> to false on the consumer).</p>
<p>Note: Logical and invalid offsets (e.g., OFFSET_INVALID) in
offsets will be ignored. If there are no valid offsets in
offsets the function will return successfully and no action
will be taken.</p>
<p>:param list(TopicPartition) offsets: current consumer/processing
position(offsets) for the
list of partitions.
:param object group_metadata: consumer group metadata retrieved
from the input consumer's
get_consumer_group_metadata().
:param float timeout: Amount of time to block in seconds.</p>
<p>:raises: KafkaError: Use exc.args[0].retriable() to check if the
operation may be retried, or
exc.args[0].txn_requires_abort() if the current
transaction has failed and must be aborted by calling
abort_transaction() and then start a new transaction
with begin_transaction().
Treat any other error as a fatal error.</p></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.Producer.set_sasl_credentials"><code class="name flex">
<span>def <span class="ident">set_sasl_credentials</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>.. py:function:: set_sasl_credentials(username, password)</p>
<p>Sets the SASL credentials used for this client.
These credentials will overwrite the old ones, and will be used the next time the client needs to authenticate.
This method will not disconnect existing broker connections that have been established with the old credentials.
This method is applicable only to SASL PLAIN and SCRAM mechanisms.</p></div>
</dd>
</dl>
</dd>
<dt id="dynamicio.mixins.with_kafka.WithKafka"><code class="flex name class">
<span>class <span class="ident">WithKafka</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles I/O operations for Kafka.</p>
<h2 id="args">Args</h2>
<ul>
<li>options:<ul>
<li>Standard: Keyword-arguments passed to the KafkaProducer constructor (see <code>KafkaProducer.DEFAULT_CONFIG.keys()</code>).</li>
<li>
<p>Additional Options:</p>
<ul>
<li>
<p><code>key_generator: Callable[[Any, Mapping], T]</code>: defines the keying policy to be used for sending keyed-messages to Kafka. It is a <code>Callable</code> that takes a
<code>tuple(idx, row)</code> and returns a string that will serve as the message's key, invoked prior to serialising the key. It defaults to the dataframe's index
(which may not be composed of unique values or string type keys). It goes hand in hand with the default <code>key-serialiser</code>, which assumes that the keys
are strings and encode's them as such.</p>
</li>
<li>
<p><code>key_serializer: Callable[T, bytes]</code>: Custom key serialiser; if not provided, a default key-serializer will be used, applied on a string-key (unless key is None).</p>
</li>
</ul>
<p>N.B. Providing a custom key-generator that generates a non-string key is best provided alongside a custom key-serializer best suited to handle the custom key-type.</p>
<ul>
<li>
<p><code>document_transformer: Callable[[Mapping[Any, Any]</code>: Manipulates the messages/rows sent to Kafka as values. It is
a <code>Callable</code> taking a <code>Mapping</code> as its only
argument and return a <code>Mapping</code>, then this callable will be invoked prior to serializing each document. This can be used, for example, to add metadata to each
document that will be written to the target
Kafka topic.</p>
</li>
<li>
<p><code>value_serializer: Callable[Mapping, bytes]</code>: Custom value serialiser; if not provided, a default value-serializer will be used applied on a Mapping..</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; # Given
&gt;&gt;&gt; keyed_test_df = pd.DataFrame.from_records(
&gt;&gt;&gt;     [
&gt;&gt;&gt;         [&quot;key-01&quot;, &quot;cm_1&quot;, &quot;id_1&quot;, 1000, &quot;ABC&quot;],
&gt;&gt;&gt;         [&quot;key-02&quot;, &quot;cm_2&quot;, &quot;id_2&quot;, 1000, &quot;ABC&quot;],
&gt;&gt;&gt;         [&quot;key-03&quot;, &quot;cm_3&quot;, &quot;id_3&quot;, 1000, &quot;ABC&quot;],
&gt;&gt;&gt;     ],
&gt;&gt;&gt;     columns=[&quot;key&quot;, &quot;id&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;],
&gt;&gt;&gt; ).set_index(&quot;key&quot;)
&gt;&gt;&gt;
&gt;&gt;&gt; kafka_cloud_config = IOConfig(
&gt;&gt;&gt;     path_to_source_yaml=(os.path.join(constants.TEST_RESOURCES, &quot;processed.yaml&quot;)),
&gt;&gt;&gt;     env_identifier=&quot;CLOUD&quot;,
&gt;&gt;&gt;     dynamic_vars=constants,
&gt;&gt;&gt; ).get(source_key=&quot;WRITE_TO_KAFKA_JSON&quot;)
&gt;&gt;&gt;
&gt;&gt;&gt; write_kafka_io = WriteKafkaIO(kafka_cloud_config, key_generator=lambda key, _: key, document_transformer=lambda doc: doc[&quot;new_field&quot;]=&quot;new_value&quot;)
&gt;&gt;&gt;
&gt;&gt;&gt; # When
&gt;&gt;&gt; with patch.object(mixins, &quot;KafkaProducer&quot;) as mock__kafka_producer:
&gt;&gt;&gt;     mock__kafka_producer.DEFAULT_CONFIG = KafkaProducer.DEFAULT_CONFIG
&gt;&gt;&gt;     mock_producer = MockKafkaProducer()
&gt;&gt;&gt;     mock__kafka_producer.return_value = mock_producer
&gt;&gt;&gt;     write_kafka_io.write(keyed_test_df)
&gt;&gt;&gt;
&gt;&gt;&gt; # Then
&gt;&gt;&gt; assert mock_producer.my_stream == [
&gt;&gt;&gt;     {&quot;key&quot;: &quot;key-01&quot;, &quot;value&quot;: {&quot;bar&quot;: 1000, &quot;baz&quot;: &quot;ABC&quot;, &quot;foo&quot;: &quot;id_1&quot;, &quot;id&quot;: &quot;cm_1&quot;, &quot;new_field&quot;: &quot;new_value&quot;}},
&gt;&gt;&gt;     {&quot;key&quot;: &quot;key-02&quot;, &quot;value&quot;: {&quot;bar&quot;: 1000, &quot;baz&quot;: &quot;ABC&quot;, &quot;foo&quot;: &quot;id_2&quot;, &quot;id&quot;: &quot;cm_2&quot;, &quot;new_field&quot;: &quot;new_value&quot;}},
&gt;&gt;&gt;     {&quot;key&quot;: &quot;key-03&quot;, &quot;value&quot;: {&quot;bar&quot;: 1000, &quot;baz&quot;: &quot;ABC&quot;, &quot;foo&quot;: &quot;id_3&quot;, &quot;id&quot;: &quot;cm_3&quot;, &quot;new_field&quot;: &quot;new_value&quot;}},
&gt;&gt;&gt; ]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WithKafka:
    &#34;&#34;&#34;Handles I/O operations for Kafka.

    Args:
        - options:
            - Standard: Keyword-arguments passed to the KafkaProducer constructor (see `KafkaProducer.DEFAULT_CONFIG.keys()`).
             - Additional Options:

                - `key_generator: Callable[[Any, Mapping], T]`: defines the keying policy to be used for sending keyed-messages to Kafka. It is a `Callable` that takes a
                `tuple(idx, row)` and returns a string that will serve as the message&#39;s key, invoked prior to serialising the key. It defaults to the dataframe&#39;s index
                (which may not be composed of unique values or string type keys). It goes hand in hand with the default `key-serialiser`, which assumes that the keys
                are strings and encode&#39;s them as such.

                - `key_serializer: Callable[T, bytes]`: Custom key serialiser; if not provided, a default key-serializer will be used, applied on a string-key (unless key is None).

                N.B. Providing a custom key-generator that generates a non-string key is best provided alongside a custom key-serializer best suited to handle the custom key-type.

                - `document_transformer: Callable[[Mapping[Any, Any]`: Manipulates the messages/rows sent to Kafka as values. It is  a `Callable` taking a `Mapping` as its only
                argument and return a `Mapping`, then this callable will be invoked prior to serializing each document. This can be used, for example, to add metadata to each
                document that will be written to the target  Kafka topic.

                - `value_serializer: Callable[Mapping, bytes]`: Custom value serialiser; if not provided, a default value-serializer will be used applied on a Mapping..

    Example:
        &gt;&gt;&gt; # Given
        &gt;&gt;&gt; keyed_test_df = pd.DataFrame.from_records(
        &gt;&gt;&gt;     [
        &gt;&gt;&gt;         [&#34;key-01&#34;, &#34;cm_1&#34;, &#34;id_1&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;         [&#34;key-02&#34;, &#34;cm_2&#34;, &#34;id_2&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;         [&#34;key-03&#34;, &#34;cm_3&#34;, &#34;id_3&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;     ],
        &gt;&gt;&gt;     columns=[&#34;key&#34;, &#34;id&#34;, &#34;foo&#34;, &#34;bar&#34;, &#34;baz&#34;],
        &gt;&gt;&gt; ).set_index(&#34;key&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; kafka_cloud_config = IOConfig(
        &gt;&gt;&gt;     path_to_source_yaml=(os.path.join(constants.TEST_RESOURCES, &#34;processed.yaml&#34;)),
        &gt;&gt;&gt;     env_identifier=&#34;CLOUD&#34;,
        &gt;&gt;&gt;     dynamic_vars=constants,
        &gt;&gt;&gt; ).get(source_key=&#34;WRITE_TO_KAFKA_JSON&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; write_kafka_io = WriteKafkaIO(kafka_cloud_config, key_generator=lambda key, _: key, document_transformer=lambda doc: doc[&#34;new_field&#34;]=&#34;new_value&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; # When
        &gt;&gt;&gt; with patch.object(mixins, &#34;KafkaProducer&#34;) as mock__kafka_producer:
        &gt;&gt;&gt;     mock__kafka_producer.DEFAULT_CONFIG = KafkaProducer.DEFAULT_CONFIG
        &gt;&gt;&gt;     mock_producer = MockKafkaProducer()
        &gt;&gt;&gt;     mock__kafka_producer.return_value = mock_producer
        &gt;&gt;&gt;     write_kafka_io.write(keyed_test_df)
        &gt;&gt;&gt;
        &gt;&gt;&gt; # Then
        &gt;&gt;&gt; assert mock_producer.my_stream == [
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-01&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_1&#34;, &#34;id&#34;: &#34;cm_1&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-02&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_2&#34;, &#34;id&#34;: &#34;cm_2&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-03&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_3&#34;, &#34;id&#34;: &#34;cm_3&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt; ]
    &#34;&#34;&#34;

    sources_config: KafkaDataEnvironment
    schema: DataframeSchema
    options: MutableMapping[str, Any]
    __kafka_config: Optional[Mapping] = None
    __producer: Optional[Producer] = None
    __key_generator: Optional[Callable[[Any, Mapping[Any, Any]], Optional[str]]] = None
    __document_transformer: Optional[Callable[[Mapping[Any, Any]], Mapping[Any, Any]]] = None
    __key_serializer: Optional[Callable[[Optional[str]], Optional[bytes]]] = None
    __value_serializer: Optional[Callable[[Mapping[Any, Any]], bytes]] = None

    # N.B.: Please refer to https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md and update this config in case of a major release change.
    VALID_CONFIG_KEYS = {
        &#34;acks&#34;,
        &#34;allow.auto.create.topics&#34;,
        &#34;api.version.fallback.ms&#34;,
        &#34;api.version.request.timeout.ms&#34;,
        &#34;api.version.request&#34;,
        &#34;auto.commit.enable&#34;,
        &#34;auto.commit.interval.ms&#34;,
        &#34;auto.offset.reset&#34;,
        &#34;background_event_cb&#34;,
        &#34;batch.size&#34;,
        &#34;batch.num.messages&#34;,
        &#34;bootstrap.servers&#34;,
        &#34;broker.address.family&#34;,
        &#34;broker.address.ttl&#34;,
        &#34;broker.version.fallback&#34;,
        &#34;builtin.features&#34;,
        &#34;check.crcs&#34;,
        &#34;client.dns.lookup&#34;,
        &#34;client.id&#34;,
        &#34;client.rack&#34;,
        &#34;closesocket_cb&#34;,
        &#34;compression.codec&#34;,
        &#34;compression.level&#34;,
        &#34;compression.type&#34;,
        &#34;connect_cb&#34;,
        &#34;connections.max.idle.ms&#34;,
        &#34;consume_cb&#34;,
        &#34;consume.callback.max.messages&#34;,
        &#34;coordinator.query.interval.ms&#34;,
        &#34;debug&#34;,
        &#34;default_topic_conf&#34;,
        &#34;delivery.report.only.error&#34;,
        &#34;dr_cb&#34;,
        &#34;dr_msg_cb&#34;,
        &#34;enable.auto.commit&#34;,
        &#34;enable.auto.offset.store&#34;,
        &#34;enable.gapless.guarantee&#34;,
        &#34;enable.idempotence&#34;,
        &#34;enable.partition.eof&#34;,
        &#34;enable.random.seed&#34;,
        &#34;enable.sasl.oauthbearer.unsecure.jwt&#34;,
        &#34;enable.ssl.certificate.verification&#34;,
        &#34;enabled_events&#34;,
        &#34;error_cb&#34;,
        &#34;fetch.error.backoff.ms&#34;,
        &#34;fetch.max.bytes&#34;,
        &#34;fetch.message.max.bytes&#34;,
        &#34;fetch.min.bytes&#34;,
        &#34;fetch.queue.backoff.ms&#34;,
        &#34;fetch.wait.max.ms&#34;,
        &#34;group.id&#34;,
        &#34;group.instance.id&#34;,
        &#34;group.protocol.type&#34;,
        &#34;group.protocol&#34;,
        &#34;group.remote.assignor&#34;,
        &#34;heartbeat.interval.ms&#34;,
        &#34;interceptors&#34;,
        &#34;internal.termination.signal&#34;,
        &#34;isolation.level&#34;,
        &#34;linger.ms&#34;,
        &#34;log_cb&#34;,
        &#34;log_level&#34;,
        &#34;log.connection.close&#34;,
        &#34;log.queue&#34;,
        &#34;log.thread.name&#34;,
        &#34;max.block.ms&#34;,
        &#34;max.in.flight.requests.per.connection&#34;,
        &#34;max.in.flight&#34;,
        &#34;max.partition.fetch.bytes&#34;,
        &#34;max.poll.interval.ms&#34;,
        &#34;max.request.size&#34;,
        &#34;message.copy.max.bytes&#34;,
        &#34;message.max.bytes&#34;,
        &#34;message.send.max.retries&#34;,
        &#34;metadata.broker.list&#34;,
        &#34;metadata.max.age.ms&#34;,
        &#34;msg_order_cmp&#34;,
        &#34;oauth_cb&#34;,
        &#34;oauthbearer_token_refresh_cb&#34;,
        &#34;offset_commit_cb&#34;,
        &#34;offset.store.method&#34;,
        &#34;offset.store.path&#34;,
        &#34;offset.store.sync.interval.ms&#34;,
        &#34;on_delivery&#34;,
        &#34;opaque&#34;,
        &#34;open_cb&#34;,
        &#34;partition.assignment.strategy&#34;,
        &#34;partitioner_cb&#34;,
        &#34;partitioner&#34;,
        &#34;plugin.library.paths&#34;,
        &#34;produce.offset.report&#34;,
        &#34;queue.buffering.backpressure.threshold&#34;,
        &#34;queue.buffering.max.kbytes&#34;,
        &#34;queue.buffering.max.messages&#34;,
        &#34;queue.buffering.max.ms&#34;,
        &#34;queued.max.messages.kbytes&#34;,
        &#34;queued.min.messages&#34;,
        &#34;rebalance_cb&#34;,
        &#34;receive.buffer.bytes&#34;,
        &#34;receive.message.max.bytes&#34;,
        &#34;reconnect.backoff.jitter.ms&#34;,
        &#34;reconnect.backoff.max.ms&#34;,
        &#34;reconnect.backoff.ms&#34;,
        &#34;request.timeout.ms&#34;,
        &#34;resolve_cb&#34;,
        &#34;retry.backoff.ms&#34;,
        &#34;sasl.kerberos.keytab&#34;,
        &#34;sasl.kerberos.kinit.cmd&#34;,
        &#34;sasl.kerberos.min.time.before.relogin&#34;,
        &#34;sasl.kerberos.principal&#34;,
        &#34;sasl.kerberos.service.name&#34;,
        &#34;sasl.mechanisms&#34;,
        &#34;sasl.oauthbearer.client.id&#34;,
        &#34;sasl.oauthbearer.client.secret&#34;,
        &#34;sasl.oauthbearer.config&#34;,
        &#34;sasl.oauthbearer.extensions&#34;,
        &#34;sasl.oauthbearer.method&#34;,
        &#34;sasl.oauthbearer.scope&#34;,
        &#34;sasl.oauthbearer.token.endpoint.url&#34;,
        &#34;sasl.password&#34;,
        &#34;sasl.username&#34;,
        &#34;security.protocol&#34;,
        &#34;send.buffer.bytes&#34;,
        &#34;session.timeout.ms&#34;,
        &#34;socket_cb&#34;,
        &#34;socket.blocking.max.ms&#34;,
        &#34;socket.connection.setup.timeout.ms&#34;,
        &#34;socket.keepalive.enable&#34;,
        &#34;socket.max.fails&#34;,
        &#34;socket.nagle.disable&#34;,
        &#34;socket.timeout.ms&#34;,
        &#34;ssl_engine_callback_data&#34;,
        &#34;ssl.ca.certificate.stores&#34;,
        &#34;ssl.ca.location&#34;,
        &#34;ssl.certificate.location&#34;,
        &#34;ssl.certificate.verify_cb&#34;,
        &#34;ssl.cipher.suites&#34;,
        &#34;ssl.crl.location&#34;,
        &#34;ssl.curves.list&#34;,
        &#34;ssl.endpoint.identification.algorithm&#34;,
        &#34;ssl.engine.id&#34;,
        &#34;ssl.engine.location&#34;,
        &#34;ssl.key.location&#34;,
        &#34;ssl.key.password&#34;,
        &#34;ssl.keystore.location&#34;,
        &#34;ssl.keystore.password&#34;,
        &#34;ssl.providers&#34;,
        &#34;ssl.sigalgs.list&#34;,
        &#34;statistics.interval.ms&#34;,
        &#34;sticky.partitioning.linger.ms&#34;,
        &#34;throttle_cb&#34;,
        &#34;topic.blacklist&#34;,
        &#34;topic.metadata.propagation.max.ms&#34;,
        &#34;topic.metadata.refresh.fast.cnt&#34;,
        &#34;topic.metadata.refresh.fast.interval.ms&#34;,
        &#34;topic.metadata.refresh.interval.ms&#34;,
        &#34;topic.metadata.refresh.sparse&#34;,
        &#34;transaction.timeout.ms&#34;,
        &#34;transactional.id&#34;,
    }

    def _write_to_kafka(self, df: pd.DataFrame) -&gt; None:
        &#34;&#34;&#34;Given a dataframe where each row is a message to be sent to a Kafka Topic, iterate through all rows and send them to a Kafka topic.

         The topic is defined in `self.sources_config[&#34;kafka&#34;]` and using a kafka producer, which is flushed at the
         end of this process.

        Args:
            df: A dataframe where each row is a message to be sent to a Kafka Topic.
        &#34;&#34;&#34;
        self.populate_cls_attributes()

        if self.__producer is None:
            self.__producer = self._get_producer(self.sources_config.kafka.kafka_server, **self.options)

        self._send_messages(df=df, topic=self.sources_config.kafka.kafka_topic)

    def populate_cls_attributes(self):
        &#34;&#34;&#34;Pop dynamicio options (key_generator, document_transformer, key_serializer, value_serializer) from kafka config options.&#34;&#34;&#34;
        if self.__key_generator is None:
            self.__key_generator = lambda idx, __: idx  # default key generator uses the dataframe&#39;s index
            if self.options.get(&#34;key_generator&#34;) is not None:
                self.__key_generator = self.options.pop(&#34;key_generator&#34;)
        if self.__document_transformer is None:
            self.__document_transformer = lambda value: value
            if self.options.get(&#34;document_transformer&#34;) is not None:
                self.__document_transformer = self.options.pop(&#34;document_transformer&#34;)
        if self.__key_serializer is None:
            self.__key_serializer = self._default_key_serializer
            if self.options.get(&#34;key_serializer&#34;) is not None:
                self.__key_serializer = self.options.pop(&#34;key_serializer&#34;)
        if self.__value_serializer is None:
            self.__value_serializer = self._default_value_serializer
            if self.options.get(&#34;value_serializer&#34;) is not None:
                self.__value_serializer = self.options.pop(&#34;value_serializer&#34;)

    @utils.allow_options(VALID_CONFIG_KEYS)
    def _get_producer(self, server: str, **options: MutableMapping[str, Any]) -&gt; Producer:
        &#34;&#34;&#34;Generate and return a Kafka Producer.

        Default options are used to generate the producer. Specifically:
            - `bootstrap.servers`: Passed on through the source_config
            - `compression.type`: Uses snappy compression

        More options can be added to the producer by passing them as keyword arguments, through valid options.

        These can also override the default options.

        Args:
            server: The host name.
            **options: Keyword arguments to pass to the KafkaProducer.

        Returns:
            A Kafka producer instance.
        &#34;&#34;&#34;
        self.__kafka_config = {
            &#34;bootstrap.servers&#34;: server,
            &#34;compression.type&#34;: &#34;snappy&#34;,
            **options,
        }
        return Producer(**self.__kafka_config)

    def _send_messages(self, df: pd.DataFrame, topic: str) -&gt; None:
        logger.debug(f&#34;Sending {len(df)} messages to Kafka topic: {topic}.&#34;)
        messages = df.reset_index(drop=True).to_dict(&#34;records&#34;)

        for idx, message in zip(df.index.values, messages):
            key = self.__key_generator(idx, message)
            transformed_message = self.__document_transformer(message)
            serialized_key = self.__key_serializer(key)
            serialized_value = self.__value_serializer(transformed_message)

            self.__producer.produce(topic=topic, key=serialized_key, value=serialized_value, on_delivery=self._on_delivery)

        self.__producer.flush()

    @staticmethod
    def _on_delivery(err, msg):
        &#34;&#34;&#34;Callback for message delivery.&#34;&#34;&#34;
        if err is not None:
            raise Exception(f&#34;Message delivery failed: {err}, for message: {msg}&#34;)

    @staticmethod
    def _default_key_serializer(key: Optional[Any]) -&gt; Optional[bytes]:
        if key is not None:
            return str(key).encode(&#34;utf-8&#34;)
        return None

    @staticmethod
    def _default_value_serializer(value: Mapping) -&gt; bytes:
        return simplejson.dumps(value, ignore_nan=True).encode(&#34;utf-8&#34;)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="dynamicio.UnifiedIO" href="../index.html#dynamicio.UnifiedIO">UnifiedIO</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="dynamicio.mixins.with_kafka.WithKafka.VALID_CONFIG_KEYS"><code class="name">var <span class="ident">VALID_CONFIG_KEYS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.WithKafka.options"><code class="name">var <span class="ident">options</span> :MutableMapping[str,Any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.WithKafka.schema"><code class="name">var <span class="ident">schema</span> :<a title="dynamicio.config.pydantic.table_schema.DataframeSchema" href="../config/pydantic/table_schema.html#dynamicio.config.pydantic.table_schema.DataframeSchema">DataframeSchema</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.WithKafka.sources_config"><code class="name">var <span class="ident">sources_config</span> :<a title="dynamicio.config.pydantic.io_resources.KafkaDataEnvironment" href="../config/pydantic/io_resources.html#dynamicio.config.pydantic.io_resources.KafkaDataEnvironment">KafkaDataEnvironment</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="dynamicio.mixins.with_kafka.WithKafka.populate_cls_attributes"><code class="name flex">
<span>def <span class="ident">populate_cls_attributes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Pop dynamicio options (key_generator, document_transformer, key_serializer, value_serializer) from kafka config options.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def populate_cls_attributes(self):
    &#34;&#34;&#34;Pop dynamicio options (key_generator, document_transformer, key_serializer, value_serializer) from kafka config options.&#34;&#34;&#34;
    if self.__key_generator is None:
        self.__key_generator = lambda idx, __: idx  # default key generator uses the dataframe&#39;s index
        if self.options.get(&#34;key_generator&#34;) is not None:
            self.__key_generator = self.options.pop(&#34;key_generator&#34;)
    if self.__document_transformer is None:
        self.__document_transformer = lambda value: value
        if self.options.get(&#34;document_transformer&#34;) is not None:
            self.__document_transformer = self.options.pop(&#34;document_transformer&#34;)
    if self.__key_serializer is None:
        self.__key_serializer = self._default_key_serializer
        if self.options.get(&#34;key_serializer&#34;) is not None:
            self.__key_serializer = self.options.pop(&#34;key_serializer&#34;)
    if self.__value_serializer is None:
        self.__value_serializer = self._default_value_serializer
        if self.options.get(&#34;value_serializer&#34;) is not None:
            self.__value_serializer = self.options.pop(&#34;value_serializer&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dynamicio.mixins" href="index.html">dynamicio.mixins</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dynamicio.mixins.with_kafka.Producer" href="#dynamicio.mixins.with_kafka.Producer">Producer</a></code></h4>
<ul class="">
<li><code><a title="dynamicio.mixins.with_kafka.Producer.abort_transaction" href="#dynamicio.mixins.with_kafka.Producer.abort_transaction">abort_transaction</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.begin_transaction" href="#dynamicio.mixins.with_kafka.Producer.begin_transaction">begin_transaction</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.commit_transaction" href="#dynamicio.mixins.with_kafka.Producer.commit_transaction">commit_transaction</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.flush" href="#dynamicio.mixins.with_kafka.Producer.flush">flush</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.init_transactions" href="#dynamicio.mixins.with_kafka.Producer.init_transactions">init_transactions</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.list_topics" href="#dynamicio.mixins.with_kafka.Producer.list_topics">list_topics</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.poll" href="#dynamicio.mixins.with_kafka.Producer.poll">poll</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.produce" href="#dynamicio.mixins.with_kafka.Producer.produce">produce</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.purge" href="#dynamicio.mixins.with_kafka.Producer.purge">purge</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.send_offsets_to_transaction" href="#dynamicio.mixins.with_kafka.Producer.send_offsets_to_transaction">send_offsets_to_transaction</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.Producer.set_sasl_credentials" href="#dynamicio.mixins.with_kafka.Producer.set_sasl_credentials">set_sasl_credentials</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dynamicio.mixins.with_kafka.WithKafka" href="#dynamicio.mixins.with_kafka.WithKafka">WithKafka</a></code></h4>
<ul class="">
<li><code><a title="dynamicio.mixins.with_kafka.WithKafka.VALID_CONFIG_KEYS" href="#dynamicio.mixins.with_kafka.WithKafka.VALID_CONFIG_KEYS">VALID_CONFIG_KEYS</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.WithKafka.options" href="#dynamicio.mixins.with_kafka.WithKafka.options">options</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.WithKafka.populate_cls_attributes" href="#dynamicio.mixins.with_kafka.WithKafka.populate_cls_attributes">populate_cls_attributes</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.WithKafka.schema" href="#dynamicio.mixins.with_kafka.WithKafka.schema">schema</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.WithKafka.sources_config" href="#dynamicio.mixins.with_kafka.WithKafka.sources_config">sources_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>