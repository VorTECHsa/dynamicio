<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dynamicio.core API documentation</title>
<meta name="description" content="Implements the DynamicDataIO class which provides functionality for data: loading; sinking, and; schema validation." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dynamicio.core</code></h1>
</header>
<section id="section-intro">
<p>Implements the DynamicDataIO class which provides functionality for data: loading; sinking, and; schema validation.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Implements the DynamicDataIO class which provides functionality for data: loading; sinking, and; schema validation.&#34;&#34;&#34;
# pylint: disable=no-member
__all__ = [&#34;DynamicDataIO&#34;, &#34;SCHEMA_FROM_FILE&#34;]

import asyncio
import inspect
import re
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Mapping, MutableMapping, Optional

import pandas as pd  # type: ignore
from magic_logger import logger

from dynamicio import validations
from dynamicio.errors import CASTING_WARNING_MSG, NOTICE_MSG, ColumnsDataTypeError, MissingSchemaDefinition, SchemaNotFoundError, SchemaValidationError
from dynamicio.metrics import get_metric

SCHEMA_FROM_FILE = {&#34;schema&#34;: object()}

pool = ThreadPoolExecutor()


class DynamicDataIO:
    &#34;&#34;&#34;Given a `src.utils.dynamicio.config.IOConfig` object, it generates an object with access to a series of methods for cloud I/O operations and data validations.

    Example:
       &gt;&gt;&gt; input_sources_config = IOConfig(
       &gt;&gt;&gt;     &#34;path_to/input.yaml&#34;,
       &gt;&gt;&gt;     os.getenv(&#34;ENVIRONMENT&#34;,default=&#34;LOCAL&#34;)
       &gt;&gt;&gt; )
       &gt;&gt;&gt;
       &gt;&gt;&gt; class CmVolumesIO(DynamicDataIO):
       &gt;&gt;&gt;     schema = {
       &gt;&gt;&gt;         &#34;id&#34;: &#34;object&#34;,
       &gt;&gt;&gt;         &#34;product_id&#34;: &#34;object&#34;,
       &gt;&gt;&gt;         &#34;tonnes&#34;: &#34;float64&#34;,
       &gt;&gt;&gt;         &#34;cubic_metres&#34;: &#34;float64&#34;,
       &gt;&gt;&gt;     }
       &gt;&gt;&gt;
       &gt;&gt;&gt;     @staticmethod
       &gt;&gt;&gt;     def validate(df: pd.DataFrame):
       &gt;&gt;&gt;         pass
       &gt;&gt;&gt;
       &gt;&gt;&gt; cm_volumes_local_mapping = input_config.get(source_key=&#34;CM_VOLUMES&#34;)
       &gt;&gt;&gt; cm_volumes_io = CmVolumesIO(cm_volumes_local_mapping, model=CmVolume)
       &gt;&gt;&gt; cm_volumes_df = cm_volumes_io.read()
    &#34;&#34;&#34;

    schema: Mapping

    def __init__(
        self,
        source_config: Mapping,
        apply_schema_validations: bool = False,
        log_schema_metrics: bool = False,
        show_casting_warnings: bool = False,
        **options: MutableMapping[str, Any],
    ):
        &#34;&#34;&#34;Class constructor.

        Args:
            source_config: Configuration to use when reading/writing data from/to a source
            apply_schema_validations: Applies schema validations on either read() or write()
            log_schema_metrics: Logs schema metrics on either read() or write()
            show_casting_warnings: Logs casting warnings on either read() or write() if set to True
            options: Any additional kwargs that may be used throughout the lifecycle of the object
        &#34;&#34;&#34;
        if type(self) is DynamicDataIO:  # pylint: disable=unidiomatic-typecheck
            raise TypeError(&#34;Abstract class DynamicDataIO cannot be used to instantiate an object...&#34;)

        self.sources_config = source_config
        self.name = self._transform_class_name_to_dataset_name(self.__class__.__name__)
        self.apply_schema_validations = apply_schema_validations
        self.log_schema_metrics = log_schema_metrics
        self.show_casting_warnings = show_casting_warnings
        self.options = self._get_options(options, source_config.get(&#34;options&#34;))
        source_name = self.sources_config.get(&#34;type&#34;)
        if self.schema is SCHEMA_FROM_FILE:
            try:
                self.schema = self.sources_config[&#34;schema&#34;]
                self.name = self.sources_config[&#34;name&#34;].upper()
                self.schema_validations = self.sources_config[&#34;validations&#34;]
                self.schema_metrics = self.sources_config[&#34;metrics&#34;]
            except KeyError as _error:
                raise SchemaNotFoundError() from _error

        assert hasattr(self, f&#34;_read_from_{source_name}&#34;) or hasattr(
            self, f&#34;_write_to_{source_name}&#34;
        ), f&#34;No method &#39;_read_from_{source_name}&#39; or &#39;_write_to_{source_name}&#39;. Have you registered a mixin for {source_name}?&#34;

    def __init_subclass__(cls):
        &#34;&#34;&#34;Ensure that all subclasses have a `schema` attribute and a `validate` method.

        Raises:
            AssertionError: If either of the attributes is not implemented
        &#34;&#34;&#34;
        if not inspect.getmodule(cls).__name__.startswith(&#34;dynamicio&#34;):
            assert &#34;schema&#34; in cls.__dict__

            if cls.schema is None or (cls.schema is not SCHEMA_FROM_FILE and len(cls.schema) == 0):
                raise ValueError(f&#34;schema for class {cls} cannot be None or empty...&#34;)

    async def async_read(self):
        &#34;&#34;&#34;Allows the use of asyncio to concurrently read files in memory.

        Returns:
            A pandas dataframe or an iterable.
        &#34;&#34;&#34;
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(pool, self.read)

    def read(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Reads data source and returns a schema validated dataframe (by means of _apply_schema).

        Returns:
            A pandas dataframe or an iterable.
        &#34;&#34;&#34;
        source_name = self.sources_config.get(&#34;type&#34;)
        df = getattr(self, f&#34;_read_from_{source_name}&#34;)()

        df = self._apply_schema(df)
        if self.apply_schema_validations:
            self.validate_from_schema(df)
        if self.log_schema_metrics:
            self.log_metrics_from_schema(df)

        return df

    async def async_write(self, df: pd.DataFrame):
        &#34;&#34;&#34;Allows the use of asyncio to concurrently write files out.

        Args:
            df: The data to be written
        &#34;&#34;&#34;
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(pool, self.write, df)

    def write(self, df: pd.DataFrame):
        &#34;&#34;&#34;Sink data to a given source based on the sources_config.

        Args:
            df: The data to be written
        &#34;&#34;&#34;
        source_name = self.sources_config.get(&#34;type&#34;)
        if set(list(df.columns)) != set(self.schema.keys()):  # pylint: disable=E1101
            columns = [column for column in df.columns.to_list() if column in self.schema.keys()]
            df = df[columns]

        if self.apply_schema_validations:
            self.validate_from_schema(df)
        if self.log_schema_metrics:
            self.log_metrics_from_schema(df)

        getattr(self, f&#34;_write_to_{source_name}&#34;)(self._apply_schema(df))

    def validate_from_schema(self, df: pd.DataFrame) -&gt; &#34;DynamicDataIO&#34;:
        &#34;&#34;&#34;Validates a dataframe based on the validations present in its schema definition.

        All validations are checked and if any of them fails, a `SchemaValidationError` is raised.

        Args:
            df:

        Returns:
             self (to allow for method chaining).

        Raises:
            SchemaValidationError: if any of the validations failed. The `message` attribute of
                the exception object is a `List[str]`, where each element is the name of a
                validation that failed.
        &#34;&#34;&#34;
        if not hasattr(self, &#34;schema_validations&#34;):
            raise MissingSchemaDefinition(self.__class__)

        failed_validations = {}
        for column in self.schema_validations.keys():
            for validation in self.schema_validations[column].keys():
                if self.schema_validations[column][validation][&#34;apply&#34;] is True:
                    validation_result = getattr(validations, validation)(self.name, df, column, **self.schema_validations[column][validation][&#34;options&#34;])
                    if not validation_result.valid:
                        failed_validations[validation] = validation_result.message

        if len(failed_validations) &gt; 0:
            raise SchemaValidationError(failed_validations)

        return self

    def log_metrics_from_schema(self, df: pd.DataFrame) -&gt; &#34;DynamicDataIO&#34;:
        &#34;&#34;&#34;Calculates and logs metrics based on the metrics present in its schema definition.

        Args:
            df: A dataframe for which metrics are generated and logged

        Returns:
             self (to allow for method chaining).
        &#34;&#34;&#34;
        if not hasattr(self, &#34;schema_metrics&#34;):
            raise MissingSchemaDefinition(self.__class__)

        for column in self.schema_metrics.keys():
            for metric in self.schema_metrics[column]:
                get_metric(metric)(self.name, df, column)()  # type: ignore

        return self

    def _apply_schema(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Called by the `self.read()` and the `self._write_to_local()` methods.

        Contrasts a dataframe&#39;s read from a given source against the class&#39;s schema dictionary,
        checking that columns are the same (by means of _has_columns and _has_valid_dtypes). Then,
        check if the columns are fine, it further validates if the types of columns conform to the
        expected schema. Finally, if schema types are different, then it attempts to apply schema;
        if possible then the schema validation is successful.

        Args:
            df: A pandas dataframe.

        Returns:
            A schema validated dataframe.
        &#34;&#34;&#34;
        if not self._has_valid_dtypes(df):
            raise ColumnsDataTypeError()
        return df

    @staticmethod
    def _transform_class_name_to_dataset_name(string_to_transform: str) -&gt; str:
        &#34;&#34;&#34;Called by the init function to fetch dataset names from class name.

        Used to create dataset name from class name, turns camel case into upper snake case.
        For example: &#39;ThisNameABC&#39; -&gt; &#39;THIS_NAME_ABC&#39;.
        &#34;&#34;&#34;
        words = re.findall(r&#34;\d[A-Z]+|[A-Z]?[a-z\d]+|[A-Z]{2,}(?=[A-Z][a-z]|\d|\W|$)|\d+|[A-Z]{2,}|[A-Z]&#34;, string_to_transform)
        return &#34;_&#34;.join(map(str.lower, words)).upper()

    def _has_valid_dtypes(self, df: pd.DataFrame) -&gt; bool:
        &#34;&#34;&#34;Checks if `df` has the expected dtypes defined in `schema`.

        Schema is a dictionary object where keys are column names and values are dtypes in string format as returned by e.g.
        `df[column].dtype.name`.

        This function issues `error` level logs describing the first column that caused the check to fail.

        It is assumed that `df` only has the columns defined in `schema`.

        Args:
            df:

        Returns:
            bool - `True` if `df` has the given dtypes, `False` otherwise
        &#34;&#34;&#34;
        dtypes = df.dtypes

        for column_name, expected_dtype in self.schema.items():
            found_dtype = dtypes[column_name].name
            if found_dtype != expected_dtype:
                if self.show_casting_warnings:
                    logger.info(f&#34;Expected: &#39;{expected_dtype}&#39; dtype for {self.name}[&#39;{column_name}]&#39;, found &#39;{found_dtype}&#39;&#34;)
                try:
                    if len(set([type(v) for v in df[column_name].values])) &gt; 1:  # pylint: disable=consider-using-set-comprehension
                        logger.warning(CASTING_WARNING_MSG.format(column_name, expected_dtype, found_dtype))  # pylint: disable=logging-format-interpolation
                        logger.info(NOTICE_MSG.format(column_name))  # pylint: disable=logging-format-interpolation
                    df.loc[:, column_name] = df[column_name].astype(self.schema[column_name])
                except (ValueError, TypeError):
                    logger.error(f&#34;ValueError: Tried casting column {self.name}[&#39;{column_name}]&#39; to &#39;{expected_dtype}&#39; &#34; f&#34;from &#39;{found_dtype}&#39;, but failed&#34;)
                    return False
        return True

    @staticmethod
    def _get_options(options_from_code: MutableMapping[str, Any], options_from_resource_definition: Optional[Mapping[str, Any]]) -&gt; MutableMapping[str, Any]:
        &#34;&#34;&#34;Retrieves options either from code or from a resource-definition.

        Options are merged if they are provided by both sources, while in the case of conflicts, the options from the code
        take precedence.

        Args:
            options_from_code (Optional[Mapping])
            options_from_resource_definition (Optional[Mapping])

        Returns:
            [Optional[Mapping]]: options that are going to be used
        &#34;&#34;&#34;
        if options_from_resource_definition:
            return {**options_from_resource_definition, **options_from_code}
        return options_from_code</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dynamicio.core.DynamicDataIO"><code class="flex name class">
<span>class <span class="ident">DynamicDataIO</span></span>
<span>(</span><span>source_config: Mapping[~KT, +VT_co], apply_schema_validations: bool = False, log_schema_metrics: bool = False, show_casting_warnings: bool = False, **options: MutableMapping[str, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Given a <code>src.utils.dynamicio.config.IOConfig</code> object, it generates an object with access to a series of methods for cloud I/O operations and data validations.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; input_sources_config = IOConfig(
&gt;&gt;&gt;     &quot;path_to/input.yaml&quot;,
&gt;&gt;&gt;     os.getenv(&quot;ENVIRONMENT&quot;,default=&quot;LOCAL&quot;)
&gt;&gt;&gt; )
&gt;&gt;&gt;
&gt;&gt;&gt; class CmVolumesIO(DynamicDataIO):
&gt;&gt;&gt;     schema = {
&gt;&gt;&gt;         &quot;id&quot;: &quot;object&quot;,
&gt;&gt;&gt;         &quot;product_id&quot;: &quot;object&quot;,
&gt;&gt;&gt;         &quot;tonnes&quot;: &quot;float64&quot;,
&gt;&gt;&gt;         &quot;cubic_metres&quot;: &quot;float64&quot;,
&gt;&gt;&gt;     }
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def validate(df: pd.DataFrame):
&gt;&gt;&gt;         pass
&gt;&gt;&gt;
&gt;&gt;&gt; cm_volumes_local_mapping = input_config.get(source_key=&quot;CM_VOLUMES&quot;)
&gt;&gt;&gt; cm_volumes_io = CmVolumesIO(cm_volumes_local_mapping, model=CmVolume)
&gt;&gt;&gt; cm_volumes_df = cm_volumes_io.read()
</code></pre>
<p>Class constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source_config</code></strong></dt>
<dd>Configuration to use when reading/writing data from/to a source</dd>
<dt><strong><code>apply_schema_validations</code></strong></dt>
<dd>Applies schema validations on either read() or write()</dd>
<dt><strong><code>log_schema_metrics</code></strong></dt>
<dd>Logs schema metrics on either read() or write()</dd>
<dt><strong><code>show_casting_warnings</code></strong></dt>
<dd>Logs casting warnings on either read() or write() if set to True</dd>
<dt><strong><code>options</code></strong></dt>
<dd>Any additional kwargs that may be used throughout the lifecycle of the object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DynamicDataIO:
    &#34;&#34;&#34;Given a `src.utils.dynamicio.config.IOConfig` object, it generates an object with access to a series of methods for cloud I/O operations and data validations.

    Example:
       &gt;&gt;&gt; input_sources_config = IOConfig(
       &gt;&gt;&gt;     &#34;path_to/input.yaml&#34;,
       &gt;&gt;&gt;     os.getenv(&#34;ENVIRONMENT&#34;,default=&#34;LOCAL&#34;)
       &gt;&gt;&gt; )
       &gt;&gt;&gt;
       &gt;&gt;&gt; class CmVolumesIO(DynamicDataIO):
       &gt;&gt;&gt;     schema = {
       &gt;&gt;&gt;         &#34;id&#34;: &#34;object&#34;,
       &gt;&gt;&gt;         &#34;product_id&#34;: &#34;object&#34;,
       &gt;&gt;&gt;         &#34;tonnes&#34;: &#34;float64&#34;,
       &gt;&gt;&gt;         &#34;cubic_metres&#34;: &#34;float64&#34;,
       &gt;&gt;&gt;     }
       &gt;&gt;&gt;
       &gt;&gt;&gt;     @staticmethod
       &gt;&gt;&gt;     def validate(df: pd.DataFrame):
       &gt;&gt;&gt;         pass
       &gt;&gt;&gt;
       &gt;&gt;&gt; cm_volumes_local_mapping = input_config.get(source_key=&#34;CM_VOLUMES&#34;)
       &gt;&gt;&gt; cm_volumes_io = CmVolumesIO(cm_volumes_local_mapping, model=CmVolume)
       &gt;&gt;&gt; cm_volumes_df = cm_volumes_io.read()
    &#34;&#34;&#34;

    schema: Mapping

    def __init__(
        self,
        source_config: Mapping,
        apply_schema_validations: bool = False,
        log_schema_metrics: bool = False,
        show_casting_warnings: bool = False,
        **options: MutableMapping[str, Any],
    ):
        &#34;&#34;&#34;Class constructor.

        Args:
            source_config: Configuration to use when reading/writing data from/to a source
            apply_schema_validations: Applies schema validations on either read() or write()
            log_schema_metrics: Logs schema metrics on either read() or write()
            show_casting_warnings: Logs casting warnings on either read() or write() if set to True
            options: Any additional kwargs that may be used throughout the lifecycle of the object
        &#34;&#34;&#34;
        if type(self) is DynamicDataIO:  # pylint: disable=unidiomatic-typecheck
            raise TypeError(&#34;Abstract class DynamicDataIO cannot be used to instantiate an object...&#34;)

        self.sources_config = source_config
        self.name = self._transform_class_name_to_dataset_name(self.__class__.__name__)
        self.apply_schema_validations = apply_schema_validations
        self.log_schema_metrics = log_schema_metrics
        self.show_casting_warnings = show_casting_warnings
        self.options = self._get_options(options, source_config.get(&#34;options&#34;))
        source_name = self.sources_config.get(&#34;type&#34;)
        if self.schema is SCHEMA_FROM_FILE:
            try:
                self.schema = self.sources_config[&#34;schema&#34;]
                self.name = self.sources_config[&#34;name&#34;].upper()
                self.schema_validations = self.sources_config[&#34;validations&#34;]
                self.schema_metrics = self.sources_config[&#34;metrics&#34;]
            except KeyError as _error:
                raise SchemaNotFoundError() from _error

        assert hasattr(self, f&#34;_read_from_{source_name}&#34;) or hasattr(
            self, f&#34;_write_to_{source_name}&#34;
        ), f&#34;No method &#39;_read_from_{source_name}&#39; or &#39;_write_to_{source_name}&#39;. Have you registered a mixin for {source_name}?&#34;

    def __init_subclass__(cls):
        &#34;&#34;&#34;Ensure that all subclasses have a `schema` attribute and a `validate` method.

        Raises:
            AssertionError: If either of the attributes is not implemented
        &#34;&#34;&#34;
        if not inspect.getmodule(cls).__name__.startswith(&#34;dynamicio&#34;):
            assert &#34;schema&#34; in cls.__dict__

            if cls.schema is None or (cls.schema is not SCHEMA_FROM_FILE and len(cls.schema) == 0):
                raise ValueError(f&#34;schema for class {cls} cannot be None or empty...&#34;)

    async def async_read(self):
        &#34;&#34;&#34;Allows the use of asyncio to concurrently read files in memory.

        Returns:
            A pandas dataframe or an iterable.
        &#34;&#34;&#34;
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(pool, self.read)

    def read(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Reads data source and returns a schema validated dataframe (by means of _apply_schema).

        Returns:
            A pandas dataframe or an iterable.
        &#34;&#34;&#34;
        source_name = self.sources_config.get(&#34;type&#34;)
        df = getattr(self, f&#34;_read_from_{source_name}&#34;)()

        df = self._apply_schema(df)
        if self.apply_schema_validations:
            self.validate_from_schema(df)
        if self.log_schema_metrics:
            self.log_metrics_from_schema(df)

        return df

    async def async_write(self, df: pd.DataFrame):
        &#34;&#34;&#34;Allows the use of asyncio to concurrently write files out.

        Args:
            df: The data to be written
        &#34;&#34;&#34;
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(pool, self.write, df)

    def write(self, df: pd.DataFrame):
        &#34;&#34;&#34;Sink data to a given source based on the sources_config.

        Args:
            df: The data to be written
        &#34;&#34;&#34;
        source_name = self.sources_config.get(&#34;type&#34;)
        if set(list(df.columns)) != set(self.schema.keys()):  # pylint: disable=E1101
            columns = [column for column in df.columns.to_list() if column in self.schema.keys()]
            df = df[columns]

        if self.apply_schema_validations:
            self.validate_from_schema(df)
        if self.log_schema_metrics:
            self.log_metrics_from_schema(df)

        getattr(self, f&#34;_write_to_{source_name}&#34;)(self._apply_schema(df))

    def validate_from_schema(self, df: pd.DataFrame) -&gt; &#34;DynamicDataIO&#34;:
        &#34;&#34;&#34;Validates a dataframe based on the validations present in its schema definition.

        All validations are checked and if any of them fails, a `SchemaValidationError` is raised.

        Args:
            df:

        Returns:
             self (to allow for method chaining).

        Raises:
            SchemaValidationError: if any of the validations failed. The `message` attribute of
                the exception object is a `List[str]`, where each element is the name of a
                validation that failed.
        &#34;&#34;&#34;
        if not hasattr(self, &#34;schema_validations&#34;):
            raise MissingSchemaDefinition(self.__class__)

        failed_validations = {}
        for column in self.schema_validations.keys():
            for validation in self.schema_validations[column].keys():
                if self.schema_validations[column][validation][&#34;apply&#34;] is True:
                    validation_result = getattr(validations, validation)(self.name, df, column, **self.schema_validations[column][validation][&#34;options&#34;])
                    if not validation_result.valid:
                        failed_validations[validation] = validation_result.message

        if len(failed_validations) &gt; 0:
            raise SchemaValidationError(failed_validations)

        return self

    def log_metrics_from_schema(self, df: pd.DataFrame) -&gt; &#34;DynamicDataIO&#34;:
        &#34;&#34;&#34;Calculates and logs metrics based on the metrics present in its schema definition.

        Args:
            df: A dataframe for which metrics are generated and logged

        Returns:
             self (to allow for method chaining).
        &#34;&#34;&#34;
        if not hasattr(self, &#34;schema_metrics&#34;):
            raise MissingSchemaDefinition(self.__class__)

        for column in self.schema_metrics.keys():
            for metric in self.schema_metrics[column]:
                get_metric(metric)(self.name, df, column)()  # type: ignore

        return self

    def _apply_schema(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Called by the `self.read()` and the `self._write_to_local()` methods.

        Contrasts a dataframe&#39;s read from a given source against the class&#39;s schema dictionary,
        checking that columns are the same (by means of _has_columns and _has_valid_dtypes). Then,
        check if the columns are fine, it further validates if the types of columns conform to the
        expected schema. Finally, if schema types are different, then it attempts to apply schema;
        if possible then the schema validation is successful.

        Args:
            df: A pandas dataframe.

        Returns:
            A schema validated dataframe.
        &#34;&#34;&#34;
        if not self._has_valid_dtypes(df):
            raise ColumnsDataTypeError()
        return df

    @staticmethod
    def _transform_class_name_to_dataset_name(string_to_transform: str) -&gt; str:
        &#34;&#34;&#34;Called by the init function to fetch dataset names from class name.

        Used to create dataset name from class name, turns camel case into upper snake case.
        For example: &#39;ThisNameABC&#39; -&gt; &#39;THIS_NAME_ABC&#39;.
        &#34;&#34;&#34;
        words = re.findall(r&#34;\d[A-Z]+|[A-Z]?[a-z\d]+|[A-Z]{2,}(?=[A-Z][a-z]|\d|\W|$)|\d+|[A-Z]{2,}|[A-Z]&#34;, string_to_transform)
        return &#34;_&#34;.join(map(str.lower, words)).upper()

    def _has_valid_dtypes(self, df: pd.DataFrame) -&gt; bool:
        &#34;&#34;&#34;Checks if `df` has the expected dtypes defined in `schema`.

        Schema is a dictionary object where keys are column names and values are dtypes in string format as returned by e.g.
        `df[column].dtype.name`.

        This function issues `error` level logs describing the first column that caused the check to fail.

        It is assumed that `df` only has the columns defined in `schema`.

        Args:
            df:

        Returns:
            bool - `True` if `df` has the given dtypes, `False` otherwise
        &#34;&#34;&#34;
        dtypes = df.dtypes

        for column_name, expected_dtype in self.schema.items():
            found_dtype = dtypes[column_name].name
            if found_dtype != expected_dtype:
                if self.show_casting_warnings:
                    logger.info(f&#34;Expected: &#39;{expected_dtype}&#39; dtype for {self.name}[&#39;{column_name}]&#39;, found &#39;{found_dtype}&#39;&#34;)
                try:
                    if len(set([type(v) for v in df[column_name].values])) &gt; 1:  # pylint: disable=consider-using-set-comprehension
                        logger.warning(CASTING_WARNING_MSG.format(column_name, expected_dtype, found_dtype))  # pylint: disable=logging-format-interpolation
                        logger.info(NOTICE_MSG.format(column_name))  # pylint: disable=logging-format-interpolation
                    df.loc[:, column_name] = df[column_name].astype(self.schema[column_name])
                except (ValueError, TypeError):
                    logger.error(f&#34;ValueError: Tried casting column {self.name}[&#39;{column_name}]&#39; to &#39;{expected_dtype}&#39; &#34; f&#34;from &#39;{found_dtype}&#39;, but failed&#34;)
                    return False
        return True

    @staticmethod
    def _get_options(options_from_code: MutableMapping[str, Any], options_from_resource_definition: Optional[Mapping[str, Any]]) -&gt; MutableMapping[str, Any]:
        &#34;&#34;&#34;Retrieves options either from code or from a resource-definition.

        Options are merged if they are provided by both sources, while in the case of conflicts, the options from the code
        take precedence.

        Args:
            options_from_code (Optional[Mapping])
            options_from_resource_definition (Optional[Mapping])

        Returns:
            [Optional[Mapping]]: options that are going to be used
        &#34;&#34;&#34;
        if options_from_resource_definition:
            return {**options_from_resource_definition, **options_from_code}
        return options_from_code</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="dynamicio.UnifiedIO" href="index.html#dynamicio.UnifiedIO">UnifiedIO</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="dynamicio.core.DynamicDataIO.schema"><code class="name">var <span class="ident">schema</span> : Mapping[~KT, +VT_co]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="dynamicio.core.DynamicDataIO.async_read"><code class="name flex">
<span>async def <span class="ident">async_read</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Allows the use of asyncio to concurrently read files in memory.</p>
<h2 id="returns">Returns</h2>
<p>A pandas dataframe or an iterable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_read(self):
    &#34;&#34;&#34;Allows the use of asyncio to concurrently read files in memory.

    Returns:
        A pandas dataframe or an iterable.
    &#34;&#34;&#34;
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(pool, self.read)</code></pre>
</details>
</dd>
<dt id="dynamicio.core.DynamicDataIO.async_write"><code class="name flex">
<span>async def <span class="ident">async_write</span></span>(<span>self, df: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Allows the use of asyncio to concurrently write files out.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>The data to be written</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_write(self, df: pd.DataFrame):
    &#34;&#34;&#34;Allows the use of asyncio to concurrently write files out.

    Args:
        df: The data to be written
    &#34;&#34;&#34;
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(pool, self.write, df)</code></pre>
</details>
</dd>
<dt id="dynamicio.core.DynamicDataIO.log_metrics_from_schema"><code class="name flex">
<span>def <span class="ident">log_metrics_from_schema</span></span>(<span>self, df: pandas.core.frame.DataFrame) ‑> <a title="dynamicio.core.DynamicDataIO" href="#dynamicio.core.DynamicDataIO">DynamicDataIO</a></span>
</code></dt>
<dd>
<div class="desc"><p>Calculates and logs metrics based on the metrics present in its schema definition.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>A dataframe for which metrics are generated and logged</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>self (to allow for method chaining).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_metrics_from_schema(self, df: pd.DataFrame) -&gt; &#34;DynamicDataIO&#34;:
    &#34;&#34;&#34;Calculates and logs metrics based on the metrics present in its schema definition.

    Args:
        df: A dataframe for which metrics are generated and logged

    Returns:
         self (to allow for method chaining).
    &#34;&#34;&#34;
    if not hasattr(self, &#34;schema_metrics&#34;):
        raise MissingSchemaDefinition(self.__class__)

    for column in self.schema_metrics.keys():
        for metric in self.schema_metrics[column]:
            get_metric(metric)(self.name, df, column)()  # type: ignore

    return self</code></pre>
</details>
</dd>
<dt id="dynamicio.core.DynamicDataIO.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Reads data source and returns a schema validated dataframe (by means of _apply_schema).</p>
<h2 id="returns">Returns</h2>
<p>A pandas dataframe or an iterable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Reads data source and returns a schema validated dataframe (by means of _apply_schema).

    Returns:
        A pandas dataframe or an iterable.
    &#34;&#34;&#34;
    source_name = self.sources_config.get(&#34;type&#34;)
    df = getattr(self, f&#34;_read_from_{source_name}&#34;)()

    df = self._apply_schema(df)
    if self.apply_schema_validations:
        self.validate_from_schema(df)
    if self.log_schema_metrics:
        self.log_metrics_from_schema(df)

    return df</code></pre>
</details>
</dd>
<dt id="dynamicio.core.DynamicDataIO.validate_from_schema"><code class="name flex">
<span>def <span class="ident">validate_from_schema</span></span>(<span>self, df: pandas.core.frame.DataFrame) ‑> <a title="dynamicio.core.DynamicDataIO" href="#dynamicio.core.DynamicDataIO">DynamicDataIO</a></span>
</code></dt>
<dd>
<div class="desc"><p>Validates a dataframe based on the validations present in its schema definition.</p>
<p>All validations are checked and if any of them fails, a <code>SchemaValidationError</code> is raised.</p>
<h2 id="args">Args</h2>
<p>df:</p>
<h2 id="returns">Returns</h2>
<p>self (to allow for method chaining).</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>SchemaValidationError</code></dt>
<dd>if any of the validations failed. The <code>message</code> attribute of
the exception object is a <code>List[str]</code>, where each element is the name of a
validation that failed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_from_schema(self, df: pd.DataFrame) -&gt; &#34;DynamicDataIO&#34;:
    &#34;&#34;&#34;Validates a dataframe based on the validations present in its schema definition.

    All validations are checked and if any of them fails, a `SchemaValidationError` is raised.

    Args:
        df:

    Returns:
         self (to allow for method chaining).

    Raises:
        SchemaValidationError: if any of the validations failed. The `message` attribute of
            the exception object is a `List[str]`, where each element is the name of a
            validation that failed.
    &#34;&#34;&#34;
    if not hasattr(self, &#34;schema_validations&#34;):
        raise MissingSchemaDefinition(self.__class__)

    failed_validations = {}
    for column in self.schema_validations.keys():
        for validation in self.schema_validations[column].keys():
            if self.schema_validations[column][validation][&#34;apply&#34;] is True:
                validation_result = getattr(validations, validation)(self.name, df, column, **self.schema_validations[column][validation][&#34;options&#34;])
                if not validation_result.valid:
                    failed_validations[validation] = validation_result.message

    if len(failed_validations) &gt; 0:
        raise SchemaValidationError(failed_validations)

    return self</code></pre>
</details>
</dd>
<dt id="dynamicio.core.DynamicDataIO.write"><code class="name flex">
<span>def <span class="ident">write</span></span>(<span>self, df: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Sink data to a given source based on the sources_config.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>The data to be written</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write(self, df: pd.DataFrame):
    &#34;&#34;&#34;Sink data to a given source based on the sources_config.

    Args:
        df: The data to be written
    &#34;&#34;&#34;
    source_name = self.sources_config.get(&#34;type&#34;)
    if set(list(df.columns)) != set(self.schema.keys()):  # pylint: disable=E1101
        columns = [column for column in df.columns.to_list() if column in self.schema.keys()]
        df = df[columns]

    if self.apply_schema_validations:
        self.validate_from_schema(df)
    if self.log_schema_metrics:
        self.log_metrics_from_schema(df)

    getattr(self, f&#34;_write_to_{source_name}&#34;)(self._apply_schema(df))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dynamicio" href="index.html">dynamicio</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dynamicio.core.DynamicDataIO" href="#dynamicio.core.DynamicDataIO">DynamicDataIO</a></code></h4>
<ul class="">
<li><code><a title="dynamicio.core.DynamicDataIO.async_read" href="#dynamicio.core.DynamicDataIO.async_read">async_read</a></code></li>
<li><code><a title="dynamicio.core.DynamicDataIO.async_write" href="#dynamicio.core.DynamicDataIO.async_write">async_write</a></code></li>
<li><code><a title="dynamicio.core.DynamicDataIO.log_metrics_from_schema" href="#dynamicio.core.DynamicDataIO.log_metrics_from_schema">log_metrics_from_schema</a></code></li>
<li><code><a title="dynamicio.core.DynamicDataIO.read" href="#dynamicio.core.DynamicDataIO.read">read</a></code></li>
<li><code><a title="dynamicio.core.DynamicDataIO.schema" href="#dynamicio.core.DynamicDataIO.schema">schema</a></code></li>
<li><code><a title="dynamicio.core.DynamicDataIO.validate_from_schema" href="#dynamicio.core.DynamicDataIO.validate_from_schema">validate_from_schema</a></code></li>
<li><code><a title="dynamicio.core.DynamicDataIO.write" href="#dynamicio.core.DynamicDataIO.write">write</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>