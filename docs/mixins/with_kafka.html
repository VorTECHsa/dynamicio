<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dynamicio.mixins.with_kafka API documentation</title>
<meta name="description" content="This module provides mixins that are providing Kafka I/O support." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dynamicio.mixins.with_kafka</code></h1>
</header>
<section id="section-intro">
<p>This module provides mixins that are providing Kafka I/O support.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># pylint: disable=no-member, protected-access, too-few-public-methods

&#34;&#34;&#34;This module provides mixins that are providing Kafka I/O support.&#34;&#34;&#34;


from typing import Any, Callable, Iterable, Mapping, MutableMapping, Optional

import pandas as pd  # type: ignore
import simplejson
from kafka import KafkaProducer  # type: ignore
from magic_logger import logger


from dynamicio.config.pydantic import DataframeSchema, KafkaDataEnvironment
from dynamicio.mixins import utils


class WithKafka:
    &#34;&#34;&#34;Handles I/O operations for Kafka.

    Args:
        - options:
            - Standard: Keyword-arguments passed to the KafkaProducer constructor (see `KafkaProducer.DEFAULT_CONFIG.keys()`).
             - Additional Options:

                - `key_generator: Callable[[Any, Mapping], T]`: defines the keying policy to be used for sending keyed-messages to Kafka. It is a `Callable` that takes a
                `tuple(idx, row)` and returns a string that will serve as the message&#39;s key, invoked prior to serialising the key. It defaults to the dataframe&#39;s index
                (which may not be composed of unique values or string type keys). It goes hand in hand with the default `key-serialiser`, which assumes that the keys
                are strings and encode&#39;s them as such.

                - `key_serializer: Callable[T, bytes]`: Custom key serialiser; if not provided, a default key-serializer will be used, applied on a string-key (unless key is None).

                N.B. Providing a custom key-generator that generates a non-string key is best provided alongside a custom key-serializer best suited to handle the custom key-type.

                - `document_transformer: Callable[[Mapping[Any, Any]`: Manipulates the messages/rows sent to Kafka as values. It is  a `Callable` taking a `Mapping` as its only
                argument and return a `Mapping`, then this callable will be invoked prior to serializing each document. This can be used, for example, to add metadata to each
                document that will be written to the target  Kafka topic.

                - `value_serializer: Callable[Mapping, bytes]`: Custom value serialiser; if not provided, a default value-serializer will be used applied on a Mapping..

    Example:
        &gt;&gt;&gt; # Given
        &gt;&gt;&gt; keyed_test_df = pd.DataFrame.from_records(
        &gt;&gt;&gt;     [
        &gt;&gt;&gt;         [&#34;key-01&#34;, &#34;cm_1&#34;, &#34;id_1&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;         [&#34;key-02&#34;, &#34;cm_2&#34;, &#34;id_2&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;         [&#34;key-03&#34;, &#34;cm_3&#34;, &#34;id_3&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;     ],
        &gt;&gt;&gt;     columns=[&#34;key&#34;, &#34;id&#34;, &#34;foo&#34;, &#34;bar&#34;, &#34;baz&#34;],
        &gt;&gt;&gt; ).set_index(&#34;key&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; kafka_cloud_config = IOConfig(
        &gt;&gt;&gt;     path_to_source_yaml=(os.path.join(constants.TEST_RESOURCES, &#34;processed.yaml&#34;)),
        &gt;&gt;&gt;     env_identifier=&#34;CLOUD&#34;,
        &gt;&gt;&gt;     dynamic_vars=constants,
        &gt;&gt;&gt; ).get(source_key=&#34;WRITE_TO_KAFKA_JSON&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; write_kafka_io = WriteKafkaIO(kafka_cloud_config, key_generator=lambda key, _: key, document_transformer=lambda doc: doc[&#34;new_field&#34;]=&#34;new_value&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; # When
        &gt;&gt;&gt; with patch.object(mixins, &#34;KafkaProducer&#34;) as mock__kafka_producer:
        &gt;&gt;&gt;     mock__kafka_producer.DEFAULT_CONFIG = KafkaProducer.DEFAULT_CONFIG
        &gt;&gt;&gt;     mock_producer = MockKafkaProducer()
        &gt;&gt;&gt;     mock__kafka_producer.return_value = mock_producer
        &gt;&gt;&gt;     write_kafka_io.write(keyed_test_df)
        &gt;&gt;&gt;
        &gt;&gt;&gt; # Then
        &gt;&gt;&gt; assert mock_producer.my_stream == [
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-01&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_1&#34;, &#34;id&#34;: &#34;cm_1&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-02&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_2&#34;, &#34;id&#34;: &#34;cm_2&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-03&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_3&#34;, &#34;id&#34;: &#34;cm_3&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt; ]
    &#34;&#34;&#34;

    sources_config: KafkaDataEnvironment
    schema: DataframeSchema
    options: MutableMapping[str, Any]
    __kafka_config: Optional[Mapping] = None
    __producer: Optional[KafkaProducer] = None
    __key_generator: Optional[Callable[[Any, Mapping[Any, Any]], Optional[str]]] = None
    __document_transformer: Optional[Callable[[Mapping[Any, Any]], Mapping[Any, Any]]] = None

    def _write_to_kafka(self, df: pd.DataFrame) -&gt; None:
        &#34;&#34;&#34;Given a dataframe where each row is a message to be sent to a Kafka Topic, iterate through all rows and send them to a Kafka topic.

         The topic is defined in `self.sources_config[&#34;kafka&#34;]` and using a kafka producer, which is flushed at the
         end of this process.

        Args:
            df: A dataframe where each row is a message to be sent to a Kafka Topic.
        &#34;&#34;&#34;
        if self.__key_generator is None:
            self.__key_generator = lambda idx, __: idx  # default key generator uses the dataframe&#39;s index
            if self.options.get(&#34;key_generator&#34;) is not None:
                self.__key_generator = self.options.pop(&#34;key_generator&#34;)

        if self.__document_transformer is None:
            self.__document_transformer = lambda value: value
            if self.options.get(&#34;document_transformer&#34;) is not None:
                self.__document_transformer = self.options.pop(&#34;document_transformer&#34;)

        if self.__producer is None:
            self.__producer = self._get_producer(self.sources_config.kafka.kafka_server, **self.options)

        self._send_messages(df=df, topic=self.sources_config.kafka.kafka_topic)

    @utils.allow_options(KafkaProducer.DEFAULT_CONFIG.keys())
    def _get_producer(self, server: str, **options: MutableMapping[str, Any]) -&gt; KafkaProducer:
        &#34;&#34;&#34;Generate and return a Kafka Producer.

        Default options are used to generate the producer. Specifically:
            - `bootstrap_servers`: Passed on through the source_config
            - `value_serializer`: Uses a default_value_serializer defined in this mixin

        More options can be added to the producer by passing them as keyword arguments, through valid options.

        These can also override the default options.

        Args:
            server: The host name.
            **options: Keyword arguments to pass to the KafkaProducer.

        Returns:
            A Kafka producer instance.
        &#34;&#34;&#34;
        self.__kafka_config = {
            **{
                &#34;bootstrap_servers&#34;: server,
                &#34;compression_type&#34;: &#34;snappy&#34;,
                &#34;key_serializer&#34;: self._default_key_serializer,
                &#34;value_serializer&#34;: self._default_value_serializer,
            },
            **options,
        }
        return KafkaProducer(**self.__kafka_config)

    def _send_messages(self, df: pd.DataFrame, topic: str) -&gt; None:
        logger.info(f&#34;Sending {len(df)} messages to Kafka topic:{topic}.&#34;)

        messages = df.reset_index(drop=True).to_dict(&#34;records&#34;)
        for idx, message in zip(df.index.values, messages):
            self.__producer.send(topic, key=self.__key_generator(idx, message), value=self.__document_transformer(message))  # type: ignore

        self.__producer.flush()  # type: ignore

    @staticmethod
    def _default_key_serializer(key: Optional[str]) -&gt; Optional[bytes]:
        if key:
            return key.encode(&#34;utf-8&#34;)
        return None

    @staticmethod
    def _default_value_serializer(value: Mapping) -&gt; bytes:
        return simplejson.dumps(value, ignore_nan=True).encode(&#34;utf-8&#34;)

    def _read_from_kafka(self) -&gt; Iterable[Mapping]:  # type: ignore
        &#34;&#34;&#34;Read messages from a Kafka Topic and convert them to separate dataframes.

        Returns:
            Multiple dataframes, one per message read from the Kafka topic of interest.
        &#34;&#34;&#34;
        # TODO: Implement kafka reader</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dynamicio.mixins.with_kafka.WithKafka"><code class="flex name class">
<span>class <span class="ident">WithKafka</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles I/O operations for Kafka.</p>
<h2 id="args">Args</h2>
<ul>
<li>options:<ul>
<li>Standard: Keyword-arguments passed to the KafkaProducer constructor (see <code>KafkaProducer.DEFAULT_CONFIG.keys()</code>).</li>
<li>
<p>Additional Options:</p>
<ul>
<li>
<p><code>key_generator: Callable[[Any, Mapping], T]</code>: defines the keying policy to be used for sending keyed-messages to Kafka. It is a <code>Callable</code> that takes a
<code>tuple(idx, row)</code> and returns a string that will serve as the message's key, invoked prior to serialising the key. It defaults to the dataframe's index
(which may not be composed of unique values or string type keys). It goes hand in hand with the default <code>key-serialiser</code>, which assumes that the keys
are strings and encode's them as such.</p>
</li>
<li>
<p><code>key_serializer: Callable[T, bytes]</code>: Custom key serialiser; if not provided, a default key-serializer will be used, applied on a string-key (unless key is None).</p>
</li>
</ul>
<p>N.B. Providing a custom key-generator that generates a non-string key is best provided alongside a custom key-serializer best suited to handle the custom key-type.</p>
<ul>
<li>
<p><code>document_transformer: Callable[[Mapping[Any, Any]</code>: Manipulates the messages/rows sent to Kafka as values. It is
a <code>Callable</code> taking a <code>Mapping</code> as its only
argument and return a <code>Mapping</code>, then this callable will be invoked prior to serializing each document. This can be used, for example, to add metadata to each
document that will be written to the target
Kafka topic.</p>
</li>
<li>
<p><code>value_serializer: Callable[Mapping, bytes]</code>: Custom value serialiser; if not provided, a default value-serializer will be used applied on a Mapping..</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; # Given
&gt;&gt;&gt; keyed_test_df = pd.DataFrame.from_records(
&gt;&gt;&gt;     [
&gt;&gt;&gt;         [&quot;key-01&quot;, &quot;cm_1&quot;, &quot;id_1&quot;, 1000, &quot;ABC&quot;],
&gt;&gt;&gt;         [&quot;key-02&quot;, &quot;cm_2&quot;, &quot;id_2&quot;, 1000, &quot;ABC&quot;],
&gt;&gt;&gt;         [&quot;key-03&quot;, &quot;cm_3&quot;, &quot;id_3&quot;, 1000, &quot;ABC&quot;],
&gt;&gt;&gt;     ],
&gt;&gt;&gt;     columns=[&quot;key&quot;, &quot;id&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;],
&gt;&gt;&gt; ).set_index(&quot;key&quot;)
&gt;&gt;&gt;
&gt;&gt;&gt; kafka_cloud_config = IOConfig(
&gt;&gt;&gt;     path_to_source_yaml=(os.path.join(constants.TEST_RESOURCES, &quot;processed.yaml&quot;)),
&gt;&gt;&gt;     env_identifier=&quot;CLOUD&quot;,
&gt;&gt;&gt;     dynamic_vars=constants,
&gt;&gt;&gt; ).get(source_key=&quot;WRITE_TO_KAFKA_JSON&quot;)
&gt;&gt;&gt;
&gt;&gt;&gt; write_kafka_io = WriteKafkaIO(kafka_cloud_config, key_generator=lambda key, _: key, document_transformer=lambda doc: doc[&quot;new_field&quot;]=&quot;new_value&quot;)
&gt;&gt;&gt;
&gt;&gt;&gt; # When
&gt;&gt;&gt; with patch.object(mixins, &quot;KafkaProducer&quot;) as mock__kafka_producer:
&gt;&gt;&gt;     mock__kafka_producer.DEFAULT_CONFIG = KafkaProducer.DEFAULT_CONFIG
&gt;&gt;&gt;     mock_producer = MockKafkaProducer()
&gt;&gt;&gt;     mock__kafka_producer.return_value = mock_producer
&gt;&gt;&gt;     write_kafka_io.write(keyed_test_df)
&gt;&gt;&gt;
&gt;&gt;&gt; # Then
&gt;&gt;&gt; assert mock_producer.my_stream == [
&gt;&gt;&gt;     {&quot;key&quot;: &quot;key-01&quot;, &quot;value&quot;: {&quot;bar&quot;: 1000, &quot;baz&quot;: &quot;ABC&quot;, &quot;foo&quot;: &quot;id_1&quot;, &quot;id&quot;: &quot;cm_1&quot;, &quot;new_field&quot;: &quot;new_value&quot;}},
&gt;&gt;&gt;     {&quot;key&quot;: &quot;key-02&quot;, &quot;value&quot;: {&quot;bar&quot;: 1000, &quot;baz&quot;: &quot;ABC&quot;, &quot;foo&quot;: &quot;id_2&quot;, &quot;id&quot;: &quot;cm_2&quot;, &quot;new_field&quot;: &quot;new_value&quot;}},
&gt;&gt;&gt;     {&quot;key&quot;: &quot;key-03&quot;, &quot;value&quot;: {&quot;bar&quot;: 1000, &quot;baz&quot;: &quot;ABC&quot;, &quot;foo&quot;: &quot;id_3&quot;, &quot;id&quot;: &quot;cm_3&quot;, &quot;new_field&quot;: &quot;new_value&quot;}},
&gt;&gt;&gt; ]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WithKafka:
    &#34;&#34;&#34;Handles I/O operations for Kafka.

    Args:
        - options:
            - Standard: Keyword-arguments passed to the KafkaProducer constructor (see `KafkaProducer.DEFAULT_CONFIG.keys()`).
             - Additional Options:

                - `key_generator: Callable[[Any, Mapping], T]`: defines the keying policy to be used for sending keyed-messages to Kafka. It is a `Callable` that takes a
                `tuple(idx, row)` and returns a string that will serve as the message&#39;s key, invoked prior to serialising the key. It defaults to the dataframe&#39;s index
                (which may not be composed of unique values or string type keys). It goes hand in hand with the default `key-serialiser`, which assumes that the keys
                are strings and encode&#39;s them as such.

                - `key_serializer: Callable[T, bytes]`: Custom key serialiser; if not provided, a default key-serializer will be used, applied on a string-key (unless key is None).

                N.B. Providing a custom key-generator that generates a non-string key is best provided alongside a custom key-serializer best suited to handle the custom key-type.

                - `document_transformer: Callable[[Mapping[Any, Any]`: Manipulates the messages/rows sent to Kafka as values. It is  a `Callable` taking a `Mapping` as its only
                argument and return a `Mapping`, then this callable will be invoked prior to serializing each document. This can be used, for example, to add metadata to each
                document that will be written to the target  Kafka topic.

                - `value_serializer: Callable[Mapping, bytes]`: Custom value serialiser; if not provided, a default value-serializer will be used applied on a Mapping..

    Example:
        &gt;&gt;&gt; # Given
        &gt;&gt;&gt; keyed_test_df = pd.DataFrame.from_records(
        &gt;&gt;&gt;     [
        &gt;&gt;&gt;         [&#34;key-01&#34;, &#34;cm_1&#34;, &#34;id_1&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;         [&#34;key-02&#34;, &#34;cm_2&#34;, &#34;id_2&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;         [&#34;key-03&#34;, &#34;cm_3&#34;, &#34;id_3&#34;, 1000, &#34;ABC&#34;],
        &gt;&gt;&gt;     ],
        &gt;&gt;&gt;     columns=[&#34;key&#34;, &#34;id&#34;, &#34;foo&#34;, &#34;bar&#34;, &#34;baz&#34;],
        &gt;&gt;&gt; ).set_index(&#34;key&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; kafka_cloud_config = IOConfig(
        &gt;&gt;&gt;     path_to_source_yaml=(os.path.join(constants.TEST_RESOURCES, &#34;processed.yaml&#34;)),
        &gt;&gt;&gt;     env_identifier=&#34;CLOUD&#34;,
        &gt;&gt;&gt;     dynamic_vars=constants,
        &gt;&gt;&gt; ).get(source_key=&#34;WRITE_TO_KAFKA_JSON&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; write_kafka_io = WriteKafkaIO(kafka_cloud_config, key_generator=lambda key, _: key, document_transformer=lambda doc: doc[&#34;new_field&#34;]=&#34;new_value&#34;)
        &gt;&gt;&gt;
        &gt;&gt;&gt; # When
        &gt;&gt;&gt; with patch.object(mixins, &#34;KafkaProducer&#34;) as mock__kafka_producer:
        &gt;&gt;&gt;     mock__kafka_producer.DEFAULT_CONFIG = KafkaProducer.DEFAULT_CONFIG
        &gt;&gt;&gt;     mock_producer = MockKafkaProducer()
        &gt;&gt;&gt;     mock__kafka_producer.return_value = mock_producer
        &gt;&gt;&gt;     write_kafka_io.write(keyed_test_df)
        &gt;&gt;&gt;
        &gt;&gt;&gt; # Then
        &gt;&gt;&gt; assert mock_producer.my_stream == [
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-01&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_1&#34;, &#34;id&#34;: &#34;cm_1&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-02&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_2&#34;, &#34;id&#34;: &#34;cm_2&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt;     {&#34;key&#34;: &#34;key-03&#34;, &#34;value&#34;: {&#34;bar&#34;: 1000, &#34;baz&#34;: &#34;ABC&#34;, &#34;foo&#34;: &#34;id_3&#34;, &#34;id&#34;: &#34;cm_3&#34;, &#34;new_field&#34;: &#34;new_value&#34;}},
        &gt;&gt;&gt; ]
    &#34;&#34;&#34;

    sources_config: KafkaDataEnvironment
    schema: DataframeSchema
    options: MutableMapping[str, Any]
    __kafka_config: Optional[Mapping] = None
    __producer: Optional[KafkaProducer] = None
    __key_generator: Optional[Callable[[Any, Mapping[Any, Any]], Optional[str]]] = None
    __document_transformer: Optional[Callable[[Mapping[Any, Any]], Mapping[Any, Any]]] = None

    def _write_to_kafka(self, df: pd.DataFrame) -&gt; None:
        &#34;&#34;&#34;Given a dataframe where each row is a message to be sent to a Kafka Topic, iterate through all rows and send them to a Kafka topic.

         The topic is defined in `self.sources_config[&#34;kafka&#34;]` and using a kafka producer, which is flushed at the
         end of this process.

        Args:
            df: A dataframe where each row is a message to be sent to a Kafka Topic.
        &#34;&#34;&#34;
        if self.__key_generator is None:
            self.__key_generator = lambda idx, __: idx  # default key generator uses the dataframe&#39;s index
            if self.options.get(&#34;key_generator&#34;) is not None:
                self.__key_generator = self.options.pop(&#34;key_generator&#34;)

        if self.__document_transformer is None:
            self.__document_transformer = lambda value: value
            if self.options.get(&#34;document_transformer&#34;) is not None:
                self.__document_transformer = self.options.pop(&#34;document_transformer&#34;)

        if self.__producer is None:
            self.__producer = self._get_producer(self.sources_config.kafka.kafka_server, **self.options)

        self._send_messages(df=df, topic=self.sources_config.kafka.kafka_topic)

    @utils.allow_options(KafkaProducer.DEFAULT_CONFIG.keys())
    def _get_producer(self, server: str, **options: MutableMapping[str, Any]) -&gt; KafkaProducer:
        &#34;&#34;&#34;Generate and return a Kafka Producer.

        Default options are used to generate the producer. Specifically:
            - `bootstrap_servers`: Passed on through the source_config
            - `value_serializer`: Uses a default_value_serializer defined in this mixin

        More options can be added to the producer by passing them as keyword arguments, through valid options.

        These can also override the default options.

        Args:
            server: The host name.
            **options: Keyword arguments to pass to the KafkaProducer.

        Returns:
            A Kafka producer instance.
        &#34;&#34;&#34;
        self.__kafka_config = {
            **{
                &#34;bootstrap_servers&#34;: server,
                &#34;compression_type&#34;: &#34;snappy&#34;,
                &#34;key_serializer&#34;: self._default_key_serializer,
                &#34;value_serializer&#34;: self._default_value_serializer,
            },
            **options,
        }
        return KafkaProducer(**self.__kafka_config)

    def _send_messages(self, df: pd.DataFrame, topic: str) -&gt; None:
        logger.info(f&#34;Sending {len(df)} messages to Kafka topic:{topic}.&#34;)

        messages = df.reset_index(drop=True).to_dict(&#34;records&#34;)
        for idx, message in zip(df.index.values, messages):
            self.__producer.send(topic, key=self.__key_generator(idx, message), value=self.__document_transformer(message))  # type: ignore

        self.__producer.flush()  # type: ignore

    @staticmethod
    def _default_key_serializer(key: Optional[str]) -&gt; Optional[bytes]:
        if key:
            return key.encode(&#34;utf-8&#34;)
        return None

    @staticmethod
    def _default_value_serializer(value: Mapping) -&gt; bytes:
        return simplejson.dumps(value, ignore_nan=True).encode(&#34;utf-8&#34;)

    def _read_from_kafka(self) -&gt; Iterable[Mapping]:  # type: ignore
        &#34;&#34;&#34;Read messages from a Kafka Topic and convert them to separate dataframes.

        Returns:
            Multiple dataframes, one per message read from the Kafka topic of interest.
        &#34;&#34;&#34;
        # TODO: Implement kafka reader</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="dynamicio.UnifiedIO" href="../index.html#dynamicio.UnifiedIO">UnifiedIO</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="dynamicio.mixins.with_kafka.WithKafka.options"><code class="name">var <span class="ident">options</span> : MutableMapping[str, Any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.WithKafka.schema"><code class="name">var <span class="ident">schema</span> : <a title="dynamicio.config.pydantic.table_schema.DataframeSchema" href="../config/pydantic/table_schema.html#dynamicio.config.pydantic.table_schema.DataframeSchema">DataframeSchema</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_kafka.WithKafka.sources_config"><code class="name">var <span class="ident">sources_config</span> : <a title="dynamicio.config.pydantic.io_resources.KafkaDataEnvironment" href="../config/pydantic/io_resources.html#dynamicio.config.pydantic.io_resources.KafkaDataEnvironment">KafkaDataEnvironment</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dynamicio.mixins" href="index.html">dynamicio.mixins</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dynamicio.mixins.with_kafka.WithKafka" href="#dynamicio.mixins.with_kafka.WithKafka">WithKafka</a></code></h4>
<ul class="">
<li><code><a title="dynamicio.mixins.with_kafka.WithKafka.options" href="#dynamicio.mixins.with_kafka.WithKafka.options">options</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.WithKafka.schema" href="#dynamicio.mixins.with_kafka.WithKafka.schema">schema</a></code></li>
<li><code><a title="dynamicio.mixins.with_kafka.WithKafka.sources_config" href="#dynamicio.mixins.with_kafka.WithKafka.sources_config">sources_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>