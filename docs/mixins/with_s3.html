<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dynamicio.mixins.with_s3 API documentation</title>
<meta name="description" content="This module provides mixins that are providing S3 I/O support." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dynamicio.mixins.with_s3</code></h1>
</header>
<section id="section-intro">
<p>This module provides mixins that are providing S3 I/O support.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># pylint: disable=no-member, protected-access, too-few-public-methods

&#34;&#34;&#34;This module provides mixins that are providing S3 I/O support.&#34;&#34;&#34;

import os
import tempfile
from contextlib import contextmanager
from typing import Generator

import boto3  # type: ignore
import pandas as pd  # type: ignore
from awscli.clidriver import create_clidriver  # type: ignore
from magic_logger import logger


from . import (
    utils,
    with_local,
)


def awscli_runner(*cmd: str):
    &#34;&#34;&#34;Runs the awscli command provided.

    Args:
        *cmd: A list of args used in the command.

    Raises:
        A runtime error exception is raised if download fails.

    Example:

        &gt;&gt;&gt; awscli_runner(&#34;s3&#34;, &#34;sync&#34;, &#34;s3://mock-bucket/mock-key&#34;, &#34;.&#34;)
    &#34;&#34;&#34;
    # Run
    exit_code = create_clidriver().main(cmd)

    if exit_code &gt; 0:
        raise RuntimeError(f&#34;AWS CLI exited with code {exit_code}&#34;)


class WithS3PathPrefix(with_local.WithLocal):
    &#34;&#34;&#34;Handles I/O operations for AWS S3; implements read operations only.

    This mixin assumes that the directories it reads from will only contain a single file-type.
    &#34;&#34;&#34;

    def _write_to_s3_path_prefix(self, df: pd.DataFrame):
        &#34;&#34;&#34;Write a DataFrame to an S3 path prefix.

        The configuration object is expected to have the following keys:
            - `bucket`
            - `path_prefix`
            - `file_type`

        Args:
            df (pd.DataFrame): the DataFrame to be written to S3

        Raises:
            ValueError: In case `path_prefix` is missing from config
            ValueError: In case the `partition_cols` arg is missing while trying to write a parquet file
        &#34;&#34;&#34;
        s3_config = self.sources_config[&#34;s3&#34;]
        if &#34;path_prefix&#34; not in s3_config:
            raise ValueError(&#34;`path_prefix` is required to write multiple files to an S3 key&#34;)

        file_type = s3_config[&#34;file_type&#34;]
        if file_type != &#34;parquet&#34;:
            raise ValueError(f&#34;File type not supported: {file_type}, only parquet files can be written to an S3 key&#34;)
        if &#34;partition_cols&#34; not in self.options:
            raise ValueError(&#34;`partition_cols` is required as an option to write partitioned parquet files to S3&#34;)

        bucket = s3_config[&#34;bucket&#34;]
        path_prefix = s3_config[&#34;path_prefix&#34;]
        full_path_prefix = utils.resolve_template(f&#34;s3://{bucket}/{path_prefix}&#34;, self.options)

        with tempfile.TemporaryDirectory() as temp_dir:
            self._write_parquet_file(df, temp_dir, **self.options)
            awscli_runner(
                &#34;s3&#34;,
                &#34;sync&#34;,
                temp_dir,
                full_path_prefix,
                &#34;--acl&#34;,
                &#34;bucket-owner-full-control&#34;,
                &#34;--only-show-errors&#34;,
                &#34;--exact-timestamps&#34;,
            )

    def _read_from_s3_path_prefix(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read all files under a path prefix from an S3 bucket as a `DataFrame`.

        The configuration object is expected to have the following keys:
            - `bucket`
            - `path_prefix`
            - `file_type`

        To actually read the file, a method is dynamically invoked by name, using
        &#34;_read_{file_type}_path_prefix&#34;.

        Returns:
            DataFrame
        &#34;&#34;&#34;
        s3_config = self.sources_config[&#34;s3&#34;]
        if &#34;path_prefix&#34; not in s3_config:
            raise ValueError(&#34;`path_prefix` is required to read multiple files from an S3 source&#34;)

        file_type = s3_config[&#34;file_type&#34;]
        if file_type not in {&#34;parquet&#34;, &#34;csv&#34;, &#34;hdf&#34;, &#34;json&#34;}:
            raise ValueError(f&#34;File type not supported: {file_type}&#34;)

        bucket = s3_config[&#34;bucket&#34;]
        path_prefix = s3_config[&#34;path_prefix&#34;]
        full_path_prefix = utils.resolve_template(f&#34;s3://{bucket}/{path_prefix}&#34;, self.options)

        # The `no_disk_space` option should be used only when reading a subset of columns from S3
        if self.options.pop(&#34;no_disk_space&#34;, False) and file_type == &#34;parquet&#34;:
            return self._read_parquet_file(full_path_prefix, self.schema, **self.options)

        with tempfile.TemporaryDirectory() as temp_dir:
            # aws-cli is shown to be up to 6 times faster when downloading the complete dataset from S3 than using the boto3
            # client or pandas directly. This is because aws-cli uses the parallel downloader, which is much faster than the
            # boto3 client.
            awscli_runner(
                &#34;s3&#34;,
                &#34;sync&#34;,
                full_path_prefix,
                temp_dir,
                &#34;--acl&#34;,
                &#34;bucket-owner-full-control&#34;,
                &#34;--only-show-errors&#34;,
                &#34;--exact-timestamps&#34;,
            )

            dfs = []
            for file in os.listdir(temp_dir):
                df = getattr(self, f&#34;_read_{file_type}_file&#34;)(os.path.join(temp_dir, file), self.schema, **self.options)  # type: ignore
                if len(df) &gt; 0:
                    dfs.append(df)

            return pd.concat(dfs, ignore_index=True)


class WithS3File(with_local.WithLocal):
    &#34;&#34;&#34;Handles I/O operations for AWS S3.

    All files are persisted to disk first using boto3 as this has proven to be faster than reading them into memory.
    Note that reading things into memory is available for csv, json and parquet types only. Unfortunately, until support
    for generic buffer is added to read_hdf, we need to download and persists the file to disk first anyway.

    Options:
        no_disk_space: If `True`, then s3fs + fsspec will be used to read data directly into memory.
    &#34;&#34;&#34;

    boto3_client = boto3.client(&#34;s3&#34;)

    @contextmanager
    def _s3_reader(self, s3_bucket: str, s3_key: str) -&gt; Generator:
        &#34;&#34;&#34;Contextmanager to abstract reading different file types in S3.

        Args:
            s3_bucket: The S3 bucket from where to read the file.
            s3_key: The file-path to the target file to be read.

        Returns:
            The local file path from where the file can be read, once it has been downloaded there by the boto3.client.

        &#34;&#34;&#34;
        with tempfile.NamedTemporaryFile(&#34;wb&#34;) as target_file:
            # Download the file from S3
            self.boto3_client.download_fileobj(s3_bucket, s3_key, target_file)
            # Yield local file path to body of `with` statement
            target_file.flush()
            yield target_file

    @contextmanager
    def _s3_writer(self, s3_bucket: str, s3_key: str) -&gt; Generator:
        &#34;&#34;&#34;Contextmanager to abstract loading different file types to S3.

        Args:
            s3_bucket: The S3 bucket to upload the file to.
            s3_key: The file-path where the target file should be uploaded to.

        Returns:
            The local file path where to actually write the file, to be read and uploaded by boto3.client.
        &#34;&#34;&#34;
        with tempfile.NamedTemporaryFile(&#34;wb&#34;) as target_file:
            # Yield local file path to body of `with` statement
            yield target_file
            target_file.flush()

            # Upload the file to S3
            self.boto3_client.upload_file(target_file.name, s3_bucket, s3_key, ExtraArgs={&#34;ACL&#34;: &#34;bucket-owner-full-control&#34;})

    def _read_from_s3_file(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read a file from an S3 bucket as a `DataFrame`.

        The configuration object is expected to have the following keys:
            - `bucket`
            - `file_path`
            - `file_type`

        To actually read the file, a method is dynamically invoked by name, using &#34;_read_{file_type}_file&#34;.

        Returns:
            DataFrame
        &#34;&#34;&#34;
        s3_config = self.sources_config[&#34;s3&#34;]
        if &#34;file_path&#34; not in s3_config:
            raise ValueError(&#34;`file_path` is required for reading a file from an S3 source&#34;)

        file_type = s3_config[&#34;file_type&#34;]
        file_path = utils.resolve_template(s3_config[&#34;file_path&#34;], self.options)
        bucket = s3_config[&#34;bucket&#34;]

        logger.info(f&#34;[s3] Started downloading: s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;)
        if file_type in [&#34;csv&#34;, &#34;json&#34;, &#34;parquet&#34;] and self.options.pop(&#34;no_disk_space&#34;, None):
            return getattr(self, f&#34;_read_{file_type}_file&#34;)(f&#34;s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;, self.schema, **self.options)  # type: ignore
        with self._s3_reader(s3_bucket=bucket, s3_key=file_path) as target_file:  # type: ignore
            return getattr(self, f&#34;_read_{file_type}_file&#34;)(target_file.name, self.schema, **self.options)  # type: ignore

    def _write_to_s3_file(self, df: pd.DataFrame):
        &#34;&#34;&#34;Write a dataframe to s3 based on the {file_type} of the config_io configuration.

        The configuration object is expected to have two keys:

            - `file_path`
            - `file_type`

        To actually write the file, a method is dynamically invoked by name, using &#34;_write_{file_type}_file&#34;.

        Args:
            df: The dataframe to be written out
        &#34;&#34;&#34;
        s3_config = self.sources_config[&#34;s3&#34;]
        file_path = utils.resolve_template(s3_config[&#34;file_path&#34;], self.options)
        file_type = s3_config[&#34;file_type&#34;]

        logger.info(f&#34;[s3] Started uploading: s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;)
        if file_type in [&#34;csv&#34;, &#34;json&#34;, &#34;parquet&#34;]:
            getattr(self, f&#34;_write_{file_type}_file&#34;)(df, f&#34;s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;, **self.options)  # type: ignore
        elif file_type == &#34;hdf&#34;:
            with self._s3_writer(s3_bucket=s3_config[&#34;bucket&#34;], s3_key=file_path) as target_file:  # type: ignore
                self._write_hdf_file(df, target_file.name, **self.options)  # type: ignore
        else:
            raise ValueError(f&#34;File type: {file_type} not supported!&#34;)
        logger.info(f&#34;[s3] Finished uploading: s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dynamicio.mixins.with_s3.awscli_runner"><code class="name flex">
<span>def <span class="ident">awscli_runner</span></span>(<span>*cmd: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the awscli command provided.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*cmd</code></strong></dt>
<dd>A list of args used in the command.</dd>
</dl>
<h2 id="raises">Raises</h2>
<p>A runtime error exception is raised if download fails.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; awscli_runner(&quot;s3&quot;, &quot;sync&quot;, &quot;s3://mock-bucket/mock-key&quot;, &quot;.&quot;)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def awscli_runner(*cmd: str):
    &#34;&#34;&#34;Runs the awscli command provided.

    Args:
        *cmd: A list of args used in the command.

    Raises:
        A runtime error exception is raised if download fails.

    Example:

        &gt;&gt;&gt; awscli_runner(&#34;s3&#34;, &#34;sync&#34;, &#34;s3://mock-bucket/mock-key&#34;, &#34;.&#34;)
    &#34;&#34;&#34;
    # Run
    exit_code = create_clidriver().main(cmd)

    if exit_code &gt; 0:
        raise RuntimeError(f&#34;AWS CLI exited with code {exit_code}&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dynamicio.mixins.with_s3.WithS3File"><code class="flex name class">
<span>class <span class="ident">WithS3File</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles I/O operations for AWS S3.</p>
<p>All files are persisted to disk first using boto3 as this has proven to be faster than reading them into memory.
Note that reading things into memory is available for csv, json and parquet types only. Unfortunately, until support
for generic buffer is added to read_hdf, we need to download and persists the file to disk first anyway.</p>
<h2 id="options">Options</h2>
<p>no_disk_space: If <code>True</code>, then s3fs + fsspec will be used to read data directly into memory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WithS3File(with_local.WithLocal):
    &#34;&#34;&#34;Handles I/O operations for AWS S3.

    All files are persisted to disk first using boto3 as this has proven to be faster than reading them into memory.
    Note that reading things into memory is available for csv, json and parquet types only. Unfortunately, until support
    for generic buffer is added to read_hdf, we need to download and persists the file to disk first anyway.

    Options:
        no_disk_space: If `True`, then s3fs + fsspec will be used to read data directly into memory.
    &#34;&#34;&#34;

    boto3_client = boto3.client(&#34;s3&#34;)

    @contextmanager
    def _s3_reader(self, s3_bucket: str, s3_key: str) -&gt; Generator:
        &#34;&#34;&#34;Contextmanager to abstract reading different file types in S3.

        Args:
            s3_bucket: The S3 bucket from where to read the file.
            s3_key: The file-path to the target file to be read.

        Returns:
            The local file path from where the file can be read, once it has been downloaded there by the boto3.client.

        &#34;&#34;&#34;
        with tempfile.NamedTemporaryFile(&#34;wb&#34;) as target_file:
            # Download the file from S3
            self.boto3_client.download_fileobj(s3_bucket, s3_key, target_file)
            # Yield local file path to body of `with` statement
            target_file.flush()
            yield target_file

    @contextmanager
    def _s3_writer(self, s3_bucket: str, s3_key: str) -&gt; Generator:
        &#34;&#34;&#34;Contextmanager to abstract loading different file types to S3.

        Args:
            s3_bucket: The S3 bucket to upload the file to.
            s3_key: The file-path where the target file should be uploaded to.

        Returns:
            The local file path where to actually write the file, to be read and uploaded by boto3.client.
        &#34;&#34;&#34;
        with tempfile.NamedTemporaryFile(&#34;wb&#34;) as target_file:
            # Yield local file path to body of `with` statement
            yield target_file
            target_file.flush()

            # Upload the file to S3
            self.boto3_client.upload_file(target_file.name, s3_bucket, s3_key, ExtraArgs={&#34;ACL&#34;: &#34;bucket-owner-full-control&#34;})

    def _read_from_s3_file(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read a file from an S3 bucket as a `DataFrame`.

        The configuration object is expected to have the following keys:
            - `bucket`
            - `file_path`
            - `file_type`

        To actually read the file, a method is dynamically invoked by name, using &#34;_read_{file_type}_file&#34;.

        Returns:
            DataFrame
        &#34;&#34;&#34;
        s3_config = self.sources_config[&#34;s3&#34;]
        if &#34;file_path&#34; not in s3_config:
            raise ValueError(&#34;`file_path` is required for reading a file from an S3 source&#34;)

        file_type = s3_config[&#34;file_type&#34;]
        file_path = utils.resolve_template(s3_config[&#34;file_path&#34;], self.options)
        bucket = s3_config[&#34;bucket&#34;]

        logger.info(f&#34;[s3] Started downloading: s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;)
        if file_type in [&#34;csv&#34;, &#34;json&#34;, &#34;parquet&#34;] and self.options.pop(&#34;no_disk_space&#34;, None):
            return getattr(self, f&#34;_read_{file_type}_file&#34;)(f&#34;s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;, self.schema, **self.options)  # type: ignore
        with self._s3_reader(s3_bucket=bucket, s3_key=file_path) as target_file:  # type: ignore
            return getattr(self, f&#34;_read_{file_type}_file&#34;)(target_file.name, self.schema, **self.options)  # type: ignore

    def _write_to_s3_file(self, df: pd.DataFrame):
        &#34;&#34;&#34;Write a dataframe to s3 based on the {file_type} of the config_io configuration.

        The configuration object is expected to have two keys:

            - `file_path`
            - `file_type`

        To actually write the file, a method is dynamically invoked by name, using &#34;_write_{file_type}_file&#34;.

        Args:
            df: The dataframe to be written out
        &#34;&#34;&#34;
        s3_config = self.sources_config[&#34;s3&#34;]
        file_path = utils.resolve_template(s3_config[&#34;file_path&#34;], self.options)
        file_type = s3_config[&#34;file_type&#34;]

        logger.info(f&#34;[s3] Started uploading: s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;)
        if file_type in [&#34;csv&#34;, &#34;json&#34;, &#34;parquet&#34;]:
            getattr(self, f&#34;_write_{file_type}_file&#34;)(df, f&#34;s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;, **self.options)  # type: ignore
        elif file_type == &#34;hdf&#34;:
            with self._s3_writer(s3_bucket=s3_config[&#34;bucket&#34;], s3_key=file_path) as target_file:  # type: ignore
                self._write_hdf_file(df, target_file.name, **self.options)  # type: ignore
        else:
            raise ValueError(f&#34;File type: {file_type} not supported!&#34;)
        logger.info(f&#34;[s3] Finished uploading: s3://{s3_config[&#39;bucket&#39;]}/{file_path}&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="dynamicio.mixins.with_local.WithLocal" href="with_local.html#dynamicio.mixins.with_local.WithLocal">WithLocal</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="dynamicio.UnifiedIO" href="../index.html#dynamicio.UnifiedIO">UnifiedIO</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="dynamicio.mixins.with_s3.WithS3File.boto3_client"><code class="name">var <span class="ident">boto3_client</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_s3.WithS3File.options"><code class="name">var <span class="ident">options</span> : MutableMapping[str, Any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_s3.WithS3File.schema"><code class="name">var <span class="ident">schema</span> : Mapping[~KT, +VT_co]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_s3.WithS3File.sources_config"><code class="name">var <span class="ident">sources_config</span> : Mapping[~KT, +VT_co]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="dynamicio.mixins.with_s3.WithS3PathPrefix"><code class="flex name class">
<span>class <span class="ident">WithS3PathPrefix</span></span>
</code></dt>
<dd>
<div class="desc"><p>Handles I/O operations for AWS S3; implements read operations only.</p>
<p>This mixin assumes that the directories it reads from will only contain a single file-type.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WithS3PathPrefix(with_local.WithLocal):
    &#34;&#34;&#34;Handles I/O operations for AWS S3; implements read operations only.

    This mixin assumes that the directories it reads from will only contain a single file-type.
    &#34;&#34;&#34;

    def _write_to_s3_path_prefix(self, df: pd.DataFrame):
        &#34;&#34;&#34;Write a DataFrame to an S3 path prefix.

        The configuration object is expected to have the following keys:
            - `bucket`
            - `path_prefix`
            - `file_type`

        Args:
            df (pd.DataFrame): the DataFrame to be written to S3

        Raises:
            ValueError: In case `path_prefix` is missing from config
            ValueError: In case the `partition_cols` arg is missing while trying to write a parquet file
        &#34;&#34;&#34;
        s3_config = self.sources_config[&#34;s3&#34;]
        if &#34;path_prefix&#34; not in s3_config:
            raise ValueError(&#34;`path_prefix` is required to write multiple files to an S3 key&#34;)

        file_type = s3_config[&#34;file_type&#34;]
        if file_type != &#34;parquet&#34;:
            raise ValueError(f&#34;File type not supported: {file_type}, only parquet files can be written to an S3 key&#34;)
        if &#34;partition_cols&#34; not in self.options:
            raise ValueError(&#34;`partition_cols` is required as an option to write partitioned parquet files to S3&#34;)

        bucket = s3_config[&#34;bucket&#34;]
        path_prefix = s3_config[&#34;path_prefix&#34;]
        full_path_prefix = utils.resolve_template(f&#34;s3://{bucket}/{path_prefix}&#34;, self.options)

        with tempfile.TemporaryDirectory() as temp_dir:
            self._write_parquet_file(df, temp_dir, **self.options)
            awscli_runner(
                &#34;s3&#34;,
                &#34;sync&#34;,
                temp_dir,
                full_path_prefix,
                &#34;--acl&#34;,
                &#34;bucket-owner-full-control&#34;,
                &#34;--only-show-errors&#34;,
                &#34;--exact-timestamps&#34;,
            )

    def _read_from_s3_path_prefix(self) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Read all files under a path prefix from an S3 bucket as a `DataFrame`.

        The configuration object is expected to have the following keys:
            - `bucket`
            - `path_prefix`
            - `file_type`

        To actually read the file, a method is dynamically invoked by name, using
        &#34;_read_{file_type}_path_prefix&#34;.

        Returns:
            DataFrame
        &#34;&#34;&#34;
        s3_config = self.sources_config[&#34;s3&#34;]
        if &#34;path_prefix&#34; not in s3_config:
            raise ValueError(&#34;`path_prefix` is required to read multiple files from an S3 source&#34;)

        file_type = s3_config[&#34;file_type&#34;]
        if file_type not in {&#34;parquet&#34;, &#34;csv&#34;, &#34;hdf&#34;, &#34;json&#34;}:
            raise ValueError(f&#34;File type not supported: {file_type}&#34;)

        bucket = s3_config[&#34;bucket&#34;]
        path_prefix = s3_config[&#34;path_prefix&#34;]
        full_path_prefix = utils.resolve_template(f&#34;s3://{bucket}/{path_prefix}&#34;, self.options)

        # The `no_disk_space` option should be used only when reading a subset of columns from S3
        if self.options.pop(&#34;no_disk_space&#34;, False) and file_type == &#34;parquet&#34;:
            return self._read_parquet_file(full_path_prefix, self.schema, **self.options)

        with tempfile.TemporaryDirectory() as temp_dir:
            # aws-cli is shown to be up to 6 times faster when downloading the complete dataset from S3 than using the boto3
            # client or pandas directly. This is because aws-cli uses the parallel downloader, which is much faster than the
            # boto3 client.
            awscli_runner(
                &#34;s3&#34;,
                &#34;sync&#34;,
                full_path_prefix,
                temp_dir,
                &#34;--acl&#34;,
                &#34;bucket-owner-full-control&#34;,
                &#34;--only-show-errors&#34;,
                &#34;--exact-timestamps&#34;,
            )

            dfs = []
            for file in os.listdir(temp_dir):
                df = getattr(self, f&#34;_read_{file_type}_file&#34;)(os.path.join(temp_dir, file), self.schema, **self.options)  # type: ignore
                if len(df) &gt; 0:
                    dfs.append(df)

            return pd.concat(dfs, ignore_index=True)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="dynamicio.mixins.with_local.WithLocal" href="with_local.html#dynamicio.mixins.with_local.WithLocal">WithLocal</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="dynamicio.UnifiedIO" href="../index.html#dynamicio.UnifiedIO">UnifiedIO</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="dynamicio.mixins.with_s3.WithS3PathPrefix.options"><code class="name">var <span class="ident">options</span> : MutableMapping[str, Any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_s3.WithS3PathPrefix.schema"><code class="name">var <span class="ident">schema</span> : Mapping[~KT, +VT_co]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="dynamicio.mixins.with_s3.WithS3PathPrefix.sources_config"><code class="name">var <span class="ident">sources_config</span> : Mapping[~KT, +VT_co]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dynamicio.mixins" href="index.html">dynamicio.mixins</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dynamicio.mixins.with_s3.awscli_runner" href="#dynamicio.mixins.with_s3.awscli_runner">awscli_runner</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dynamicio.mixins.with_s3.WithS3File" href="#dynamicio.mixins.with_s3.WithS3File">WithS3File</a></code></h4>
<ul class="">
<li><code><a title="dynamicio.mixins.with_s3.WithS3File.boto3_client" href="#dynamicio.mixins.with_s3.WithS3File.boto3_client">boto3_client</a></code></li>
<li><code><a title="dynamicio.mixins.with_s3.WithS3File.options" href="#dynamicio.mixins.with_s3.WithS3File.options">options</a></code></li>
<li><code><a title="dynamicio.mixins.with_s3.WithS3File.schema" href="#dynamicio.mixins.with_s3.WithS3File.schema">schema</a></code></li>
<li><code><a title="dynamicio.mixins.with_s3.WithS3File.sources_config" href="#dynamicio.mixins.with_s3.WithS3File.sources_config">sources_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dynamicio.mixins.with_s3.WithS3PathPrefix" href="#dynamicio.mixins.with_s3.WithS3PathPrefix">WithS3PathPrefix</a></code></h4>
<ul class="">
<li><code><a title="dynamicio.mixins.with_s3.WithS3PathPrefix.options" href="#dynamicio.mixins.with_s3.WithS3PathPrefix.options">options</a></code></li>
<li><code><a title="dynamicio.mixins.with_s3.WithS3PathPrefix.schema" href="#dynamicio.mixins.with_s3.WithS3PathPrefix.schema">schema</a></code></li>
<li><code><a title="dynamicio.mixins.with_s3.WithS3PathPrefix.sources_config" href="#dynamicio.mixins.with_s3.WithS3PathPrefix.sources_config">sources_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>